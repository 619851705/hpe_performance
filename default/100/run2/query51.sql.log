15/04/07 08:49:52 WARN conf.HiveConf: HiveConf of name hive.optimize.mapjoin.mapreduce does not exist
15/04/07 08:49:52 WARN conf.HiveConf: HiveConf of name hive.heapsize does not exist
15/04/07 08:49:52 WARN conf.HiveConf: HiveConf of name hive.server2.enable.impersonation does not exist
15/04/07 08:49:52 WARN conf.HiveConf: HiveConf of name hive.auto.convert.sortmerge.join.noconditionaltask does not exist

Logging initialized using configuration in file:/etc/hive/conf/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/hdp/2.2.0.0-2041/hadoop/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/hdp/2.2.0.0-2041/hive/lib/hive-jdbc-0.14.0.2.2.0.0-2041-standalone.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
hive> use tpcds_bin_partitioned_orc_100; source query51.sql;
OK
Time taken: 2.864 seconds
Query ID = root_20150407084949_49c9df00-9d4d-4eec-9160-8de0e93767c6
Total jobs = 7
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/hdp/2.2.0.0-2041/hadoop/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/hdp/2.2.0.0-2041/hive/lib/hive-jdbc-0.14.0.2.2.0.0-2041-standalone.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
15/04/07 08:50:23 WARN conf.HiveConf: HiveConf of name hive.optimize.mapjoin.mapreduce does not exist
15/04/07 08:50:23 WARN conf.HiveConf: HiveConf of name hive.heapsize does not exist
15/04/07 08:50:23 WARN conf.HiveConf: HiveConf of name hive.server2.enable.impersonation does not exist
15/04/07 08:50:23 WARN conf.HiveConf: HiveConf of name hive.auto.convert.sortmerge.join.noconditionaltask does not exist
Execution log at: /tmp/root/root_20150407084949_49c9df00-9d4d-4eec-9160-8de0e93767c6.log
2015-04-07 08:50:24	Starting to launch local task to process map join;	maximum memory = 1065484288
2015-04-07 08:50:28	Dump the side-table for tag: 1 with group count: 366 into file: file:/tmp/root/a2da47fa-e50e-4594-a837-39f101ed1f79/hive_2015-04-07_08-49-58_989_185805373245922370-1/-local-10012/HashTable-Stage-2/MapJoin-mapfile01--.hashtable
2015-04-07 08:50:28	Uploaded 1 File to: file:/tmp/root/a2da47fa-e50e-4594-a837-39f101ed1f79/hive_2015-04-07_08-49-58_989_185805373245922370-1/-local-10012/HashTable-Stage-2/MapJoin-mapfile01--.hashtable (13499 bytes)
2015-04-07 08:50:28	End of local task; Time Taken: 4.248 sec.
Execution completed successfully
MapredLocal task succeeded
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/hdp/2.2.0.0-2041/hadoop/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/hdp/2.2.0.0-2041/hive/lib/hive-jdbc-0.14.0.2.2.0.0-2041-standalone.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
15/04/07 08:50:35 WARN conf.HiveConf: HiveConf of name hive.optimize.mapjoin.mapreduce does not exist
15/04/07 08:50:35 WARN conf.HiveConf: HiveConf of name hive.heapsize does not exist
15/04/07 08:50:35 WARN conf.HiveConf: HiveConf of name hive.server2.enable.impersonation does not exist
15/04/07 08:50:35 WARN conf.HiveConf: HiveConf of name hive.auto.convert.sortmerge.join.noconditionaltask does not exist
Execution log at: /tmp/root/root_20150407084949_49c9df00-9d4d-4eec-9160-8de0e93767c6.log
2015-04-07 08:50:36	Starting to launch local task to process map join;	maximum memory = 1065484288
2015-04-07 08:50:40	Dump the side-table for tag: 1 with group count: 366 into file: file:/tmp/root/a2da47fa-e50e-4594-a837-39f101ed1f79/hive_2015-04-07_08-49-58_989_185805373245922370-1/-local-10014/HashTable-Stage-8/MapJoin-mapfile11--.hashtable
2015-04-07 08:50:40	Uploaded 1 File to: file:/tmp/root/a2da47fa-e50e-4594-a837-39f101ed1f79/hive_2015-04-07_08-49-58_989_185805373245922370-1/-local-10014/HashTable-Stage-8/MapJoin-mapfile11--.hashtable (13499 bytes)
2015-04-07 08:50:40	End of local task; Time Taken: 4.125 sec.
Execution completed successfully
MapredLocal task succeeded
Launching Job 1 out of 7
Number of reduce tasks not specified. Estimated from input data size: 179
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1428405026382_0076, Tracking URL = http://f01hn02.hadoop:8088/proxy/application_1428405026382_0076/
Kill Command = /usr/hdp/2.2.0.0-2041/hadoop/bin/hadoop job  -kill job_1428405026382_0076
Hadoop job information for Stage-2: number of mappers: 46; number of reducers: 179
2015-04-07 08:50:57,781 Stage-2 map = 0%,  reduce = 0%
2015-04-07 08:51:05,433 Stage-2 map = 4%,  reduce = 0%, Cumulative CPU 9.66 sec
2015-04-07 08:51:06,505 Stage-2 map = 59%,  reduce = 0%, Cumulative CPU 158.72 sec
2015-04-07 08:51:07,579 Stage-2 map = 74%,  reduce = 0%, Cumulative CPU 207.09 sec
2015-04-07 08:51:08,654 Stage-2 map = 81%,  reduce = 0%, Cumulative CPU 316.02 sec
2015-04-07 08:51:10,798 Stage-2 map = 83%,  reduce = 0%, Cumulative CPU 321.27 sec
2015-04-07 08:51:11,869 Stage-2 map = 87%,  reduce = 0%, Cumulative CPU 356.26 sec
2015-04-07 08:51:14,277 Stage-2 map = 89%,  reduce = 0%, Cumulative CPU 369.94 sec
2015-04-07 08:51:15,347 Stage-2 map = 91%,  reduce = 0%, Cumulative CPU 391.58 sec
2015-04-07 08:51:16,432 Stage-2 map = 92%,  reduce = 1%, Cumulative CPU 397.42 sec
2015-04-07 08:51:17,516 Stage-2 map = 93%,  reduce = 3%, Cumulative CPU 425.03 sec
2015-04-07 08:51:18,561 Stage-2 map = 93%,  reduce = 4%, Cumulative CPU 435.71 sec
2015-04-07 08:51:19,636 Stage-2 map = 93%,  reduce = 6%, Cumulative CPU 440.5 sec
2015-04-07 08:51:20,708 Stage-2 map = 93%,  reduce = 7%, Cumulative CPU 470.65 sec
2015-04-07 08:51:21,783 Stage-2 map = 94%,  reduce = 9%, Cumulative CPU 479.99 sec
2015-04-07 08:51:22,854 Stage-2 map = 94%,  reduce = 11%, Cumulative CPU 486.38 sec
2015-04-07 08:51:23,926 Stage-2 map = 94%,  reduce = 12%, Cumulative CPU 518.3 sec
2015-04-07 08:51:24,997 Stage-2 map = 95%,  reduce = 13%, Cumulative CPU 525.46 sec
2015-04-07 08:51:26,068 Stage-2 map = 95%,  reduce = 15%, Cumulative CPU 533.94 sec
2015-04-07 08:51:27,141 Stage-2 map = 95%,  reduce = 16%, Cumulative CPU 563.82 sec
2015-04-07 08:51:29,282 Stage-2 map = 97%,  reduce = 16%, Cumulative CPU 584.06 sec
2015-04-07 08:51:30,351 Stage-2 map = 100%,  reduce = 17%, Cumulative CPU 604.06 sec
2015-04-07 08:51:31,449 Stage-2 map = 100%,  reduce = 19%, Cumulative CPU 621.11 sec
2015-04-07 08:51:32,522 Stage-2 map = 100%,  reduce = 26%, Cumulative CPU 689.14 sec
2015-04-07 08:51:33,595 Stage-2 map = 100%,  reduce = 35%, Cumulative CPU 832.41 sec
2015-04-07 08:51:34,641 Stage-2 map = 100%,  reduce = 57%, Cumulative CPU 1160.12 sec
2015-04-07 08:51:35,717 Stage-2 map = 100%,  reduce = 59%, Cumulative CPU 1175.1 sec
2015-04-07 08:51:36,786 Stage-2 map = 100%,  reduce = 60%, Cumulative CPU 1189.34 sec
2015-04-07 08:51:37,854 Stage-2 map = 100%,  reduce = 61%, Cumulative CPU 1194.54 sec
2015-04-07 08:51:39,988 Stage-2 map = 100%,  reduce = 90%, Cumulative CPU 1457.19 sec
2015-04-07 08:51:41,054 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 1547.18 sec
MapReduce Total cumulative CPU time: 25 minutes 47 seconds 180 msec
Ended Job = job_1428405026382_0076
Launching Job 2 out of 7
Number of reduce tasks not specified. Estimated from input data size: 68
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1428405026382_0077, Tracking URL = http://f01hn02.hadoop:8088/proxy/application_1428405026382_0077/
Kill Command = /usr/hdp/2.2.0.0-2041/hadoop/bin/hadoop job  -kill job_1428405026382_0077
Hadoop job information for Stage-8: number of mappers: 18; number of reducers: 68
2015-04-07 08:51:54,607 Stage-8 map = 0%,  reduce = 0%
2015-04-07 08:52:02,070 Stage-8 map = 6%,  reduce = 0%, Cumulative CPU 5.08 sec
2015-04-07 08:52:03,138 Stage-8 map = 11%,  reduce = 0%, Cumulative CPU 12.03 sec
2015-04-07 08:52:04,205 Stage-8 map = 56%,  reduce = 0%, Cumulative CPU 68.82 sec
2015-04-07 08:52:05,275 Stage-8 map = 80%,  reduce = 0%, Cumulative CPU 142.26 sec
2015-04-07 08:52:08,407 Stage-8 map = 90%,  reduce = 0%, Cumulative CPU 161.81 sec
2015-04-07 08:52:10,827 Stage-8 map = 91%,  reduce = 0%, Cumulative CPU 168.97 sec
2015-04-07 08:52:11,890 Stage-8 map = 93%,  reduce = 0%, Cumulative CPU 175.59 sec
2015-04-07 08:52:12,960 Stage-8 map = 93%,  reduce = 5%, Cumulative CPU 180.07 sec
2015-04-07 08:52:14,030 Stage-8 map = 96%,  reduce = 10%, Cumulative CPU 196.03 sec
2015-04-07 08:52:15,100 Stage-8 map = 96%,  reduce = 14%, Cumulative CPU 199.5 sec
2015-04-07 08:52:16,170 Stage-8 map = 98%,  reduce = 19%, Cumulative CPU 206.49 sec
2015-04-07 08:52:17,240 Stage-8 map = 100%,  reduce = 24%, Cumulative CPU 214.38 sec
2015-04-07 08:52:18,321 Stage-8 map = 100%,  reduce = 37%, Cumulative CPU 241.16 sec
2015-04-07 08:52:19,376 Stage-8 map = 100%,  reduce = 89%, Cumulative CPU 464.74 sec
2015-04-07 08:52:20,442 Stage-8 map = 100%,  reduce = 100%, Cumulative CPU 513.49 sec
MapReduce Total cumulative CPU time: 8 minutes 33 seconds 490 msec
Ended Job = job_1428405026382_0077
Launching Job 3 out of 7
Number of reduce tasks not specified. Estimated from input data size: 16
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1428405026382_0078, Tracking URL = http://f01hn02.hadoop:8088/proxy/application_1428405026382_0078/
Kill Command = /usr/hdp/2.2.0.0-2041/hadoop/bin/hadoop job  -kill job_1428405026382_0078
Hadoop job information for Stage-3: number of mappers: 6; number of reducers: 16
2015-04-07 08:52:27,373 Stage-3 map = 0%,  reduce = 0%
2015-04-07 08:52:35,966 Stage-3 map = 17%,  reduce = 0%, Cumulative CPU 5.29 sec
2015-04-07 08:52:38,097 Stage-3 map = 35%,  reduce = 0%, Cumulative CPU 45.16 sec
2015-04-07 08:52:41,295 Stage-3 map = 49%,  reduce = 0%, Cumulative CPU 63.24 sec
2015-04-07 08:52:43,424 Stage-3 map = 52%,  reduce = 0%, Cumulative CPU 67.05 sec
2015-04-07 08:52:44,496 Stage-3 map = 57%,  reduce = 0%, Cumulative CPU 81.51 sec
2015-04-07 08:52:45,564 Stage-3 map = 63%,  reduce = 0%, Cumulative CPU 83.93 sec
2015-04-07 08:52:46,635 Stage-3 map = 64%,  reduce = 6%, Cumulative CPU 91.26 sec
2015-04-07 08:52:47,680 Stage-3 map = 71%,  reduce = 8%, Cumulative CPU 102.89 sec
2015-04-07 08:52:49,813 Stage-3 map = 76%,  reduce = 11%, Cumulative CPU 119.03 sec
2015-04-07 08:52:51,948 Stage-3 map = 82%,  reduce = 12%, Cumulative CPU 121.84 sec
2015-04-07 08:52:53,015 Stage-3 map = 83%,  reduce = 17%, Cumulative CPU 134.65 sec
2015-04-07 08:53:03,689 Stage-3 map = 89%,  reduce = 17%, Cumulative CPU 172.89 sec
2015-04-07 08:53:04,752 Stage-3 map = 100%,  reduce = 20%, Cumulative CPU 179.03 sec
2015-04-07 08:53:05,814 Stage-3 map = 100%,  reduce = 27%, Cumulative CPU 185.26 sec
2015-04-07 08:53:07,939 Stage-3 map = 100%,  reduce = 58%, Cumulative CPU 230.33 sec
2015-04-07 08:53:09,003 Stage-3 map = 100%,  reduce = 69%, Cumulative CPU 255.26 sec
2015-04-07 08:53:11,129 Stage-3 map = 100%,  reduce = 81%, Cumulative CPU 301.91 sec
2015-04-07 08:53:12,192 Stage-3 map = 100%,  reduce = 85%, Cumulative CPU 320.91 sec
2015-04-07 08:53:13,257 Stage-3 map = 100%,  reduce = 92%, Cumulative CPU 341.36 sec
2015-04-07 08:53:14,321 Stage-3 map = 100%,  reduce = 99%, Cumulative CPU 368.68 sec
2015-04-07 08:53:15,383 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 369.67 sec
MapReduce Total cumulative CPU time: 6 minutes 9 seconds 670 msec
Ended Job = job_1428405026382_0078
Launching Job 4 out of 7
Number of reduce tasks not specified. Estimated from input data size: 7
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1428405026382_0079, Tracking URL = http://f01hn02.hadoop:8088/proxy/application_1428405026382_0079/
Kill Command = /usr/hdp/2.2.0.0-2041/hadoop/bin/hadoop job  -kill job_1428405026382_0079
Hadoop job information for Stage-9: number of mappers: 4; number of reducers: 7
2015-04-07 08:53:22,122 Stage-9 map = 0%,  reduce = 0%
2015-04-07 08:53:29,538 Stage-9 map = 25%,  reduce = 0%, Cumulative CPU 5.23 sec
2015-04-07 08:53:32,724 Stage-9 map = 50%,  reduce = 0%, Cumulative CPU 29.57 sec
2015-04-07 08:53:35,912 Stage-9 map = 72%,  reduce = 0%, Cumulative CPU 41.05 sec
2015-04-07 08:53:38,043 Stage-9 map = 78%,  reduce = 0%, Cumulative CPU 48.21 sec
2015-04-07 08:53:41,235 Stage-9 map = 81%,  reduce = 17%, Cumulative CPU 57.65 sec
2015-04-07 08:53:43,401 Stage-9 map = 90%,  reduce = 17%, Cumulative CPU 59.21 sec
2015-04-07 08:53:44,467 Stage-9 map = 92%,  reduce = 23%, Cumulative CPU 63.62 sec
2015-04-07 08:53:46,596 Stage-9 map = 92%,  reduce = 24%, Cumulative CPU 64.26 sec
2015-04-07 08:53:47,661 Stage-9 map = 92%,  reduce = 25%, Cumulative CPU 71.19 sec
2015-04-07 08:53:54,348 Stage-9 map = 100%,  reduce = 25%, Cumulative CPU 79.76 sec
2015-04-07 08:53:56,478 Stage-9 map = 100%,  reduce = 67%, Cumulative CPU 91.85 sec
2015-04-07 08:53:59,660 Stage-9 map = 100%,  reduce = 78%, Cumulative CPU 127.98 sec
2015-04-07 08:54:02,852 Stage-9 map = 100%,  reduce = 98%, Cumulative CPU 155.87 sec
2015-04-07 08:54:03,915 Stage-9 map = 100%,  reduce = 100%, Cumulative CPU 158.89 sec
MapReduce Total cumulative CPU time: 2 minutes 38 seconds 890 msec
Ended Job = job_1428405026382_0079
Launching Job 5 out of 7
Number of reduce tasks not specified. Estimated from input data size: 23
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1428405026382_0080, Tracking URL = http://f01hn02.hadoop:8088/proxy/application_1428405026382_0080/
Kill Command = /usr/hdp/2.2.0.0-2041/hadoop/bin/hadoop job  -kill job_1428405026382_0080
Hadoop job information for Stage-4: number of mappers: 8; number of reducers: 23
2015-04-07 08:54:10,566 Stage-4 map = 0%,  reduce = 0%
2015-04-07 08:54:21,151 Stage-4 map = 2%,  reduce = 0%, Cumulative CPU 63.18 sec
2015-04-07 08:54:24,346 Stage-4 map = 32%,  reduce = 0%, Cumulative CPU 93.21 sec
2015-04-07 08:54:25,408 Stage-4 map = 41%,  reduce = 0%, Cumulative CPU 94.58 sec
2015-04-07 08:54:27,536 Stage-4 map = 53%,  reduce = 0%, Cumulative CPU 116.88 sec
2015-04-07 08:54:30,719 Stage-4 map = 56%,  reduce = 0%, Cumulative CPU 138.24 sec
2015-04-07 08:54:33,906 Stage-4 map = 72%,  reduce = 0%, Cumulative CPU 160.48 sec
2015-04-07 08:54:34,972 Stage-4 map = 72%,  reduce = 1%, Cumulative CPU 161.15 sec
2015-04-07 08:54:36,039 Stage-4 map = 72%,  reduce = 11%, Cumulative CPU 167.12 sec
2015-04-07 08:54:37,102 Stage-4 map = 77%,  reduce = 13%, Cumulative CPU 185.54 sec
2015-04-07 08:54:40,299 Stage-4 map = 79%,  reduce = 13%, Cumulative CPU 203.76 sec
2015-04-07 08:54:43,500 Stage-4 map = 88%,  reduce = 13%, Cumulative CPU 223.45 sec
2015-04-07 08:54:44,565 Stage-4 map = 88%,  reduce = 17%, Cumulative CPU 226.92 sec
2015-04-07 08:54:45,629 Stage-4 map = 88%,  reduce = 20%, Cumulative CPU 240.47 sec
2015-04-07 08:54:46,696 Stage-4 map = 88%,  reduce = 21%, Cumulative CPU 241.61 sec
2015-04-07 08:54:48,795 Stage-4 map = 92%,  reduce = 23%, Cumulative CPU 252.72 sec
2015-04-07 08:54:49,857 Stage-4 map = 96%,  reduce = 23%, Cumulative CPU 254.28 sec
2015-04-07 08:54:50,920 Stage-4 map = 96%,  reduce = 27%, Cumulative CPU 255.67 sec
2015-04-07 08:54:52,261 Stage-4 map = 96%,  reduce = 29%, Cumulative CPU 260.51 sec
2015-04-07 08:54:55,447 Stage-4 map = 100%,  reduce = 29%, Cumulative CPU 266.77 sec
2015-04-07 08:54:56,510 Stage-4 map = 100%,  reduce = 30%, Cumulative CPU 269.03 sec
2015-04-07 08:54:57,575 Stage-4 map = 100%,  reduce = 58%, Cumulative CPU 301.11 sec
2015-04-07 08:54:58,638 Stage-4 map = 100%,  reduce = 63%, Cumulative CPU 313.25 sec
2015-04-07 08:54:59,689 Stage-4 map = 100%,  reduce = 70%, Cumulative CPU 347.48 sec
2015-04-07 08:55:00,752 Stage-4 map = 100%,  reduce = 77%, Cumulative CPU 411.98 sec
2015-04-07 08:55:01,815 Stage-4 map = 100%,  reduce = 79%, Cumulative CPU 423.72 sec
2015-04-07 08:55:02,878 Stage-4 map = 100%,  reduce = 87%, Cumulative CPU 459.96 sec
2015-04-07 08:55:03,940 Stage-4 map = 100%,  reduce = 96%, Cumulative CPU 502.67 sec
2015-04-07 08:55:05,007 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 523.15 sec
MapReduce Total cumulative CPU time: 8 minutes 43 seconds 150 msec
Ended Job = job_1428405026382_0080
Launching Job 6 out of 7
Number of reduce tasks not specified. Estimated from input data size: 19
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1428405026382_0081, Tracking URL = http://f01hn02.hadoop:8088/proxy/application_1428405026382_0081/
Kill Command = /usr/hdp/2.2.0.0-2041/hadoop/bin/hadoop job  -kill job_1428405026382_0081
Hadoop job information for Stage-5: number of mappers: 6; number of reducers: 19
2015-04-07 08:55:11,552 Stage-5 map = 0%,  reduce = 0%
2015-04-07 08:55:22,211 Stage-5 map = 2%,  reduce = 0%, Cumulative CPU 45.74 sec
2015-04-07 08:55:24,336 Stage-5 map = 4%,  reduce = 0%, Cumulative CPU 49.4 sec
2015-04-07 08:55:25,399 Stage-5 map = 31%,  reduce = 0%, Cumulative CPU 68.68 sec
2015-04-07 08:55:27,496 Stage-5 map = 33%,  reduce = 0%, Cumulative CPU 76.35 sec
2015-04-07 08:55:28,561 Stage-5 map = 38%,  reduce = 0%, Cumulative CPU 87.7 sec
2015-04-07 08:55:30,668 Stage-5 map = 50%,  reduce = 0%, Cumulative CPU 106.29 sec
2015-04-07 08:55:33,858 Stage-5 map = 64%,  reduce = 0%, Cumulative CPU 124.67 sec
2015-04-07 08:55:34,922 Stage-5 map = 64%,  reduce = 1%, Cumulative CPU 125.31 sec
2015-04-07 08:55:35,987 Stage-5 map = 64%,  reduce = 11%, Cumulative CPU 130.86 sec
2015-04-07 08:55:37,053 Stage-5 map = 73%,  reduce = 11%, Cumulative CPU 145.02 sec
2015-04-07 08:55:40,248 Stage-5 map = 78%,  reduce = 11%, Cumulative CPU 160.37 sec
2015-04-07 08:55:46,624 Stage-5 map = 83%,  reduce = 11%, Cumulative CPU 193.54 sec
2015-04-07 08:55:47,687 Stage-5 map = 89%,  reduce = 14%, Cumulative CPU 196.16 sec
2015-04-07 08:55:48,749 Stage-5 map = 89%,  reduce = 19%, Cumulative CPU 197.62 sec
2015-04-07 08:55:49,813 Stage-5 map = 94%,  reduce = 19%, Cumulative CPU 203.98 sec
2015-04-07 08:55:50,880 Stage-5 map = 94%,  reduce = 25%, Cumulative CPU 205.66 sec
2015-04-07 08:55:51,945 Stage-5 map = 100%,  reduce = 28%, Cumulative CPU 209.35 sec
2015-04-07 08:55:54,069 Stage-5 map = 100%,  reduce = 49%, Cumulative CPU 241.15 sec
2015-04-07 08:55:55,131 Stage-5 map = 100%,  reduce = 68%, Cumulative CPU 272.69 sec
2015-04-07 08:55:56,194 Stage-5 map = 100%,  reduce = 69%, Cumulative CPU 277.73 sec
2015-04-07 08:55:57,256 Stage-5 map = 100%,  reduce = 90%, Cumulative CPU 350.74 sec
2015-04-07 08:55:58,316 Stage-5 map = 100%,  reduce = 99%, Cumulative CPU 378.09 sec
2015-04-07 08:55:59,376 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 380.65 sec
MapReduce Total cumulative CPU time: 6 minutes 20 seconds 650 msec
Ended Job = job_1428405026382_0081
Launching Job 7 out of 7
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1428405026382_0082, Tracking URL = http://f01hn02.hadoop:8088/proxy/application_1428405026382_0082/
Kill Command = /usr/hdp/2.2.0.0-2041/hadoop/bin/hadoop job  -kill job_1428405026382_0082
Hadoop job information for Stage-6: number of mappers: 4; number of reducers: 1
2015-04-07 08:56:06,150 Stage-6 map = 0%,  reduce = 0%
2015-04-07 08:56:11,443 Stage-6 map = 25%,  reduce = 0%, Cumulative CPU 2.91 sec
2015-04-07 08:56:12,504 Stage-6 map = 100%,  reduce = 0%, Cumulative CPU 13.23 sec
2015-04-07 08:56:17,802 Stage-6 map = 100%,  reduce = 100%, Cumulative CPU 14.65 sec
MapReduce Total cumulative CPU time: 14 seconds 650 msec
Ended Job = job_1428405026382_0082
MapReduce Jobs Launched: 
Stage-Stage-2: Map: 46  Reduce: 179   Cumulative CPU: 1547.18 sec   HDFS Read: 1463615734 HDFS Write: 1055571029 SUCCESS
Stage-Stage-8: Map: 18  Reduce: 68   Cumulative CPU: 513.49 sec   HDFS Read: 412554287 HDFS Write: 457811374 SUCCESS
Stage-Stage-3: Map: 6  Reduce: 16   Cumulative CPU: 369.67 sec   HDFS Read: 1055618439 HDFS Write: 1057797693 SUCCESS
Stage-Stage-9: Map: 4  Reduce: 7   Cumulative CPU: 158.89 sec   HDFS Read: 457829634 HDFS Write: 457814754 SUCCESS
Stage-Stage-4: Map: 8  Reduce: 23   Cumulative CPU: 523.15 sec   HDFS Read: 1515619587 HDFS Write: 1231897171 SUCCESS
Stage-Stage-5: Map: 6  Reduce: 19   Cumulative CPU: 380.65 sec   HDFS Read: 1231904021 HDFS Write: 14331421 SUCCESS
Stage-Stage-6: Map: 4  Reduce: 1   Cumulative CPU: 14.65 sec   HDFS Read: 14336941 HDFS Write: 7693 SUCCESS
Total MapReduce CPU Time Spent: 58 minutes 27 seconds 680 msec
OK
1	1999-06-03	56.939998626708984	39.310001373291016	56.939998626708984	39.310001373291016
4	1999-06-04	96.8700008392334	41.66999816894531	96.8700008392334	41.66999816894531
4	1999-06-05	NULL	43.04999816417694	96.8700008392334	43.04999816417694
4	1999-06-09	163.94000053405762	NULL	163.94000053405762	155.3499974012375
4	1999-06-14	261.7700023651123	NULL	261.7700023651123	239.38999927043915
7	1999-06-02	29.899999618530273	18.020000338554382	29.899999618530273	18.020000338554382
10	1999-06-02	64.83999633789062	NULL	64.83999633789062	19.920000076293945
17	2000-01-02	139.89999771118164	56.24000144004822	139.89999771118164	56.24000144004822
17	2000-01-03	NULL	77.22000098228455	139.89999771118164	77.22000098228455
23	2000-01-03	NULL	8.130000114440918	8.550000190734863	8.130000114440918
23	2000-01-04	172.24999332427979	26.21999979019165	172.24999332427979	26.21999979019165
23	2000-01-07	245.85999393463135	109.87999963760376	245.85999393463135	109.87999963760376
23	2000-01-08	248.31999397277832	116.21999979019165	248.31999397277832	116.21999979019165
23	2000-01-09	NULL	178.97000169754028	248.31999397277832	178.97000169754028
23	2000-01-12	NULL	181.46000170707703	248.31999397277832	181.46000170707703
23	2000-01-13	372.13999366760254	248.7100007534027	372.13999366760254	248.7100007534027
23	2000-01-15	NULL	329.93000197410583	372.13999366760254	329.93000197410583
23	2000-01-17	395.1299934387207	NULL	395.1299934387207	329.93000197410583
23	2000-01-19	442.1399917602539	358.71000266075134	442.1399917602539	358.71000266075134
23	2000-01-23	NULL	439.8500020503998	442.1399917602539	439.8500020503998
23	2000-01-25	456.74999141693115	NULL	456.74999141693115	439.8500020503998
28	1999-06-02	NULL	11.720000267028809	78.18000030517578	11.720000267028809
28	1999-06-03	139.18999862670898	NULL	139.18999862670898	11.720000267028809
28	1999-06-04	NULL	30.390000343322754	139.18999862670898	30.390000343322754
28	1999-06-05	NULL	64.27999973297119	139.18999862670898	64.27999973297119
28	1999-06-07	NULL	106.40999984741211	139.18999862670898	106.40999984741211
28	1999-06-08	195.04999923706055	NULL	195.04999923706055	106.40999984741211
28	1999-06-09	NULL	108.049999833107	195.04999923706055	108.049999833107
28	1999-06-10	250.29999923706055	NULL	250.29999923706055	108.049999833107
28	1999-06-12	409.27999687194824	138.38999998569489	409.27999687194824	138.38999998569489
28	1999-06-15	NULL	178.66999876499176	409.27999687194824	178.66999876499176
28	1999-06-16	NULL	248.59999907016754	409.27999687194824	248.59999907016754
28	1999-06-17	467.93999671936035	NULL	467.93999671936035	248.59999907016754
28	1999-06-18	NULL	254.29999899864197	467.93999671936035	254.29999899864197
28	1999-06-19	NULL	349.9399983882904	467.93999671936035	349.9399983882904
34	1999-06-07	NULL	37.31999969482422	56.15000104904175	37.31999969482422
35	2000-01-02	215.65000200271606	69.1200008392334	215.65000200271606	69.1200008392334
35	2000-01-03	268.15000200271606	NULL	268.15000200271606	69.1200008392334
35	2000-01-04	NULL	167.59000205993652	268.15000200271606	167.59000205993652
35	2000-01-07	NULL	177.22000217437744	268.15000200271606	177.22000217437744
35	2000-01-08	NULL	192.00000220537186	268.15000200271606	192.00000220537186
35	2000-01-17	NULL	217.07000190019608	268.15000200271606	217.07000190019608
35	2000-01-18	271.2600018978119	NULL	271.2600018978119	217.07000190019608
35	2000-01-19	439.6400067806244	NULL	439.6400067806244	217.07000190019608
35	2000-01-20	482.91000723838806	442.68999511003494	482.91000723838806	442.68999511003494
40	1999-06-07	31.010000228881836	NULL	31.010000228881836	26.399999618530273
40	1999-06-08	31.010000228881836	NULL	31.010000228881836	26.399999618530273
43	1999-06-02	21.719999313354492	14.219999611377716	21.719999313354492	14.219999611377716
44	1999-06-03	130.9999988079071	53.34000039100647	130.9999988079071	53.34000039100647
44	1999-06-06	190.650000333786	NULL	190.650000333786	158.01000428199768
44	1999-06-07	NULL	180.15000367164612	190.650000333786	180.15000367164612
44	1999-06-08	211.0800006389618	NULL	211.0800006389618	180.15000367164612
47	2000-01-02	80.58000183105469	13.81000018119812	80.58000183105469	13.81000018119812
49	1999-06-04	106.8700008392334	16.510000228881836	106.8700008392334	16.510000228881836
49	1999-06-05	185.74000358581543	NULL	185.74000358581543	16.510000228881836
49	1999-06-06	NULL	81.85999870300293	185.74000358581543	81.85999870300293
49	1999-06-09	NULL	134.33999824523926	185.74000358581543	134.33999824523926
49	1999-06-12	NULL	165.0499973297119	185.74000358581543	165.0499973297119
50	1999-06-06	124.71999740600586	69.19000196456909	124.71999740600586	69.19000196456909
50	1999-06-07	132.9799976348877	90.59000158309937	132.9799976348877	90.59000158309937
50	1999-06-08	295.4799976348877	116.77000188827515	295.4799976348877	116.77000188827515
50	1999-06-10	NULL	116.77000188827515	295.4799976348877	116.77000188827515
50	1999-06-11	300.9099974632263	211.4700002670288	300.9099974632263	211.4700002670288
50	1999-06-12	310.76999711990356	284.10999965667725	310.76999711990356	284.10999965667725
50	1999-06-14	NULL	305.539999961853	310.76999711990356	305.539999961853
58	1999-06-05	129.30999755859375	NULL	129.30999755859375	24.059999465942383
58	1999-06-06	NULL	69.39999961853027	129.30999755859375	69.39999961853027
58	1999-06-07	174.38999938964844	NULL	174.38999938964844	69.39999961853027
58	1999-06-08	NULL	150.73000144958496	174.38999938964844	150.73000144958496
65	2000-01-02	126.56999969482422	19.610000252723694	126.56999969482422	19.610000252723694
65	2000-01-03	NULL	106.55999720096588	126.56999969482422	106.55999720096588
65	2000-01-04	NULL	117.6899973154068	126.56999969482422	117.6899973154068
65	2000-01-05	NULL	119.6899973154068	126.56999969482422	119.6899973154068
68	1999-06-20	174.24999809265137	155.65999817848206	174.24999809265137	155.65999817848206
68	1999-06-21	NULL	170.22999787330627	174.24999809265137	170.22999787330627
70	1999-06-03	99.85000085830688	57.33999824523926	99.85000085830688	57.33999824523926
70	1999-06-05	NULL	85.27999877929688	99.85000085830688	85.27999877929688
73	1999-06-03	52.36000061035156	NULL	52.36000061035156	25.5
73	1999-06-04	273.3000030517578	NULL	273.3000030517578	25.5
73	1999-06-05	NULL	33.63000011444092	273.3000030517578	33.63000011444092
73	1999-06-06	NULL	161.9099988937378	273.3000030517578	161.9099988937378
73	1999-06-07	347.69000244140625	315.8000020980835	347.69000244140625	315.8000020980835
82	1999-06-03	NULL	8.539999961853027	103.19999694824219	8.539999961853027
82	1999-06-04	106.5799970626831	75.66000270843506	106.5799970626831	75.66000270843506
82	1999-06-05	NULL	76.9900027513504	106.5799970626831	76.9900027513504
82	1999-06-07	NULL	84.71000254154205	106.5799970626831	84.71000254154205
83	2000-01-04	261.71999740600586	241.78000259399414	261.71999740600586	241.78000259399414
83	2000-01-05	293.24999809265137	282.44000244140625	293.24999809265137	282.44000244140625
89	2000-01-03	166.8800048828125	32.69000035524368	166.8800048828125	32.69000035524368
89	2000-01-05	172.19000482559204	NULL	172.19000482559204	32.69000035524368
89	2000-01-08	NULL	80.80000132322311	172.19000482559204	80.80000132322311
89	2000-01-09	NULL	120.81000059843063	172.19000482559204	120.81000059843063
89	2000-01-14	NULL	121.23000058531761	172.19000482559204	121.23000058531761
89	2000-01-15	NULL	131.80000028014183	172.19000482559204	131.80000028014183
100	1999-06-02	86.86000061035156	84.64000034332275	86.86000061035156	84.64000034332275
100	1999-06-11	287.4300060272217	NULL	287.4300060272217	208.92000114917755
100	1999-06-12	312.8500061035156	NULL	312.8500061035156	208.92000114917755
100	1999-06-13	NULL	218.01000106334686	312.8500061035156	218.01000106334686
100	1999-06-14	NULL	235.6500004529953	312.8500061035156	235.6500004529953
100	1999-06-15	NULL	255.8100003004074	312.8500061035156	255.8100003004074
Time taken: 379.909 seconds, Fetched: 100 row(s)
hive> 