START hdp2.5_llap_10000_query96_sql_2017-01-05-09-50:  Thu Jan 5 09:50:18 CST 2017
beeline --outputformat=csv2 -u jdbc:hiv2:// -i sample-queries-tpcds/testbench.settings-4g-container-tez --database tpcds_bin_partitioned_orc_10000 -f sample-queries-tpcds/query96.sql
Connecting to jdbc:hive2://h01hn02:10500/tpcds_bin_partitioned_orc_10000
Connected to: Apache Hive (version 2.1.0.2.5.1.0-43)
Driver: Hive JDBC (version 1.2.1000.2.5.1.0-43)
Transaction isolation: TRANSACTION_REPEATABLE_READ
Running init script sample-queries-tpch/testbench.settings-4g-container
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> --set ambari.hive.db.schema.name=hive;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set fs.file.impl.disable.cache=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set fs.hdfs.impl.disable.cache=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.auto.convert.join.noconditionaltask=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.auto.convert.join=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> --set hive.auto.convert.sortmerge.join.noconditionaltask=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.auto.convert.sortmerge.join=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.compactor.abortedtxn.threshold=1000;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.compactor.check.interval=300;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.compactor.delta.num.threshold=10;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.compactor.delta.pct.threshold=0.1f;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.compactor.initiator.on=false;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.compactor.worker.threads=0;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.compactor.worker.timeout=86400;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.compute.query.using.stats=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.enforce.bucketing=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.enforce.sorting=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.enforce.sortmergebucketmapjoin=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.execution.engine=mr;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.limit.pushdown.memory.usage=0.04;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.map.aggr=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.mapjoin.bucket.cache.size=10000;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.mapred.reduce.tasks.speculative.execution=false;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.metastore.cache.pinobjtypes=Table,Database,Type,FieldSchema,Order;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.metastore.client.socket.timeout=60;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.metastore.execute.setugi=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.metastore.warehouse.dir=/apps/hive/warehouse;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.optimize.bucketmapjoin.sortedmerge=false;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.optimize.bucketmapjoin=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.optimize.index.filter=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> --set hive.optimize.mapjoin.mapreduce=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.optimize.reducededuplication.min.reducer=4;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.optimize.reducededuplication=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.orc.splits.include.file.footer=false;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.security.authorization.enabled=false;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.security.metastore.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.StorageBasedAut horizationProvider;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> --set hive.semantic.analyzer.factory.impl=org.apache.hivealog.cli.HCatSemanticAnalyzerFactory;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.server2.enable.doAs=false;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.server2.tez.default.queues=default;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.server2.tez.initialize.default.sessions=false;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.server2.tez.sessions.per.default.queue=1;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.stats.autogather=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.tez.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.txn.max.open.batch=1000;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.txn.timeout=300;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.vectorized.execution.enabled=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.vectorized.groupby.checkinterval=1024;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.vectorized.groupby.flush.percent=1;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.vectorized.groupby.maxentries=1024;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> 
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- These values need to be tuned appropriately to your cluster. These examples are for reference.
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.tez.container.size=4096;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.tez.java.opts=-Xmx3800m;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.auto.convert.join.noconditionaltask.size=1252698795;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> 
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> 
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> select  count(*) as c
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> from store_sales
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>     ,household_demographics 
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>     ,time_dim, store
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> where store_sales.ss_sold_time_sk = time_dim.t_time_sk   
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>     and store_sales.ss_hdemo_sk = household_demographics.hd_demo_sk 
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>     and store_sales.ss_store_sk = store.s_store_sk
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>     and time_dim.t_hour = 8
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>     and time_dim.t_minute >= 30
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>     and household_demographics.hd_dep_count = 5
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>     and store.s_store_name = 'ese'
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> order by c
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> limit 100;
INFO  : Compiling command(queryId=hive_20170105095019_8d837f67-8795-49e4-be85-7188ff537a00): select  count(*) as c
from store_sales
    ,household_demographics 
    ,time_dim, store
where store_sales.ss_sold_time_sk = time_dim.t_time_sk   
    and store_sales.ss_hdemo_sk = household_demographics.hd_demo_sk 
    and store_sales.ss_store_sk = store.s_store_sk
    and time_dim.t_hour = 8
    and time_dim.t_minute >= 30
    and household_demographics.hd_dep_count = 5
    and store.s_store_name = 'ese'
order by c
limit 100
INFO  : Semantic Analysis Completed
INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:c, type:bigint, comment:null)], properties:null)
INFO  : Completed compiling command(queryId=hive_20170105095019_8d837f67-8795-49e4-be85-7188ff537a00); Time taken: 0.782 seconds
INFO  : Concurrency mode is disabled, not creating a lock manager
INFO  : Executing command(queryId=hive_20170105095019_8d837f67-8795-49e4-be85-7188ff537a00): select  count(*) as c
from store_sales
    ,household_demographics 
    ,time_dim, store
where store_sales.ss_sold_time_sk = time_dim.t_time_sk   
    and store_sales.ss_hdemo_sk = household_demographics.hd_demo_sk 
    and store_sales.ss_store_sk = store.s_store_sk
    and time_dim.t_hour = 8
    and time_dim.t_minute >= 30
    and household_demographics.hd_dep_count = 5
    and store.s_store_name = 'ese'
order by c
limit 100
INFO  : Query ID = hive_20170105095019_8d837f67-8795-49e4-be85-7188ff537a00
INFO  : Total jobs = 1
INFO  : Launching Job 1 out of 1
INFO  : Starting task [Stage-1:MAPRED] in serial mode
INFO  : Session is already open
INFO  : Dag name: select  count(*) as c
from store_sales...100(Stage-1)
INFO  : Setting tez.task.scale.memory.reserve-fraction to 0.30000001192092896
INFO  : Tez session was closed. Reopening...
INFO  : Session re-established.
INFO  : 

INFO  : Status: Running (Executing on YARN cluster with App id application_1483569684333_0039)

INFO  : Map 1: -/-	Map 4: -/-	Map 5: -/-	Map 6: -/-	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: -/-	Map 4: 0/1	Map 5: 0/1	Map 6: 0/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: -/-	Map 4: 0(+1)/1	Map 5: 0(+1)/1	Map 6: 0(+1)/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: -/-	Map 4: 1/1	Map 5: 0(+1)/1	Map 6: 0(+1)/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: -/-	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: -/-	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: -/-	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 0/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 0(+133)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 0(+147)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 0(+152)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 0(+429)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 0(+429,-1)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 0(+429,-2)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 0(+429,-4)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 0(+429,-5)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 0(+429,-6)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 0(+429,-7)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 0(+429,-8)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-8)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-9)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-10)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-11)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-12)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-34)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-45)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-46)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-79)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-80)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-108)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-114)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-122)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-148)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-163)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-237)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-241)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-271)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-289)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-326)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-326)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-326)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-326)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-326)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-326)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-326)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-326)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-326)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-326)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-326)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-326)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-326)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-326)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-326)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-326)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-326)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-326)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-326)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-326)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-326)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-326)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-326)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-326)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-326)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-326)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-326)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-326)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-326)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-326)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-326)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-326)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-326)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-326)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-326)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
ERROR : Status: Failed
ERROR : Dag received [DAG_TERMINATE, SERVICE_PLUGIN_ERROR] in RUNNING state.
ERROR : Error reported by TaskScheduler [[2:LLAP]][SERVICE_UNAVAILABLE] No LLAP Daemons are running
ERROR : Vertex killed, vertexName=Reducer 3, vertexId=vertex_1483569684333_0039_1_05, diagnostics=[Vertex received Kill while in RUNNING state., Vertex did not succeed due to DAG_TERMINATED, failedTasks:0 killedTasks:1, Vertex vertex_1483569684333_0039_1_05 [Reducer 3] killed/failed due to:DAG_TERMINATED]
ERROR : Vertex killed, vertexName=Reducer 2, vertexId=vertex_1483569684333_0039_1_04, diagnostics=[Vertex received Kill while in RUNNING state., Vertex did not succeed due to DAG_TERMINATED, failedTasks:0 killedTasks:1, Vertex vertex_1483569684333_0039_1_04 [Reducer 2] killed/failed due to:DAG_TERMINATED]
ERROR : Vertex killed, vertexName=Map 1, vertexId=vertex_1483569684333_0039_1_03, diagnostics=[Vertex received Kill while in RUNNING state., Vertex did not succeed due to DAG_TERMINATED, failedTasks:0 killedTasks:428, Vertex vertex_1483569684333_0039_1_03 [Map 1] killed/failed due to:DAG_TERMINATED]
ERROR : DAG did not succeed due to SERVICE_PLUGIN_ERROR. failedVertices:0 killedVertices:3
INFO  : org.apache.tez.common.counters.DAGCounter:
INFO  :    NUM_FAILED_TASKS: 326
INFO  :    NUM_KILLED_TASKS: 653
INFO  :    NUM_SUCCEEDED_TASKS: 4
INFO  :    TOTAL_LAUNCHED_TASKS: 788
INFO  :    DATA_LOCAL_TASKS: 4
INFO  :    AM_CPU_MILLISECONDS: 225710
INFO  :    AM_GC_TIME_MILLIS: 6127
INFO  : File System Counters:
INFO  :    FILE_BYTES_READ: 64
INFO  :    FILE_BYTES_WRITTEN: 11143
INFO  :    FILE_READ_OPS: 0
INFO  :    FILE_LARGE_READ_OPS: 0
INFO  :    FILE_WRITE_OPS: 0
INFO  :    HDFS_BYTES_READ: 12267163
INFO  :    HDFS_BYTES_WRITTEN: 0
INFO  :    HDFS_READ_OPS: 42
INFO  :    HDFS_LARGE_READ_OPS: 0
INFO  :    HDFS_WRITE_OPS: 0
INFO  : org.apache.tez.common.counters.TaskCounter:
INFO  :    SPILLED_RECORDS: 1
INFO  :    INPUT_RECORDS_PROCESSED: 3419
INFO  :    INPUT_SPLIT_LENGTH_BYTES: 185656419
INFO  :    OUTPUT_RECORDS: 2702
INFO  :    OUTPUT_LARGE_RECORDS: 0
INFO  :    OUTPUT_BYTES: 13510
INFO  :    OUTPUT_BYTES_WITH_OVERHEAD: 18938
INFO  :    OUTPUT_BYTES_PHYSICAL: 11087
INFO  :    ADDITIONAL_SPILLS_BYTES_WRITTEN: 0
INFO  :    ADDITIONAL_SPILLS_BYTES_READ: 0
INFO  :    ADDITIONAL_SPILL_COUNT: 0
INFO  :    SHUFFLE_CHUNK_COUNT: 1
INFO  :    SHUFFLE_BYTES_DECOMPRESSED: 0
INFO  : HIVE:
INFO  :    DESERIALIZE_ERRORS: 0
INFO  :    RECORDS_IN_Map_1: 3383108
INFO  :    RECORDS_IN_Map_4: 20000
INFO  :    RECORDS_IN_Map_5: 7200
INFO  :    RECORDS_IN_Map_6: 1500
INFO  :    RECORDS_OUT_INTERMEDIATE_Map_1: 1
INFO  :    RECORDS_OUT_INTERMEDIATE_Map_4: 1800
INFO  :    RECORDS_OUT_INTERMEDIATE_Map_5: 720
INFO  :    RECORDS_OUT_INTERMEDIATE_Map_6: 181
INFO  : TaskCounter_Map_1_INPUT_Map_4:
INFO  :    INPUT_RECORDS_PROCESSED: 0
INFO  :    SHUFFLE_BYTES_DECOMPRESSED: 0
INFO  : TaskCounter_Map_1_INPUT_Map_5:
INFO  :    INPUT_RECORDS_PROCESSED: 0
INFO  :    SHUFFLE_BYTES_DECOMPRESSED: 0
INFO  : TaskCounter_Map_1_INPUT_Map_6:
INFO  :    INPUT_RECORDS_PROCESSED: 0
INFO  :    SHUFFLE_BYTES_DECOMPRESSED: 0
INFO  : TaskCounter_Map_1_INPUT_store_sales:
INFO  :    INPUT_RECORDS_PROCESSED: 3389
INFO  :    INPUT_SPLIT_LENGTH_BYTES: 185079271
INFO  : TaskCounter_Map_1_OUTPUT_Reducer_2:
INFO  :    ADDITIONAL_SPILLS_BYTES_READ: 0
INFO  :    ADDITIONAL_SPILLS_BYTES_WRITTEN: 0
INFO  :    ADDITIONAL_SPILL_COUNT: 0
INFO  :    OUTPUT_BYTES: 5
INFO  :    OUTPUT_BYTES_PHYSICAL: 27
INFO  :    OUTPUT_BYTES_WITH_OVERHEAD: 13
INFO  :    OUTPUT_RECORDS: 1
INFO  :    SHUFFLE_CHUNK_COUNT: 1
INFO  :    SPILLED_RECORDS: 1
INFO  : TaskCounter_Map_4_INPUT_time_dim:
INFO  :    INPUT_RECORDS_PROCESSED: 20
INFO  :    INPUT_SPLIT_LENGTH_BYTES: 448579
INFO  : TaskCounter_Map_4_OUTPUT_Map_1:
INFO  :    ADDITIONAL_SPILLS_BYTES_READ: 0
INFO  :    ADDITIONAL_SPILLS_BYTES_WRITTEN: 0
INFO  :    ADDITIONAL_SPILL_COUNT: 0
INFO  :    OUTPUT_BYTES: 9000
INFO  :    OUTPUT_BYTES_PHYSICAL: 7222
INFO  :    OUTPUT_BYTES_WITH_OVERHEAD: 12606
INFO  :    OUTPUT_LARGE_RECORDS: 0
INFO  :    OUTPUT_RECORDS: 1800
INFO  :    SPILLED_RECORDS: 0
INFO  : TaskCounter_Map_5_INPUT_household_demographics:
INFO  :    INPUT_RECORDS_PROCESSED: 8
INFO  :    INPUT_SPLIT_LENGTH_BYTES: 1147
INFO  : TaskCounter_Map_5_OUTPUT_Map_1:
INFO  :    ADDITIONAL_SPILLS_BYTES_READ: 0
INFO  :    ADDITIONAL_SPILLS_BYTES_WRITTEN: 0
INFO  :    ADDITIONAL_SPILL_COUNT: 0
INFO  :    OUTPUT_BYTES: 3600
INFO  :    OUTPUT_BYTES_PHYSICAL: 3073
INFO  :    OUTPUT_BYTES_WITH_OVERHEAD: 5046
INFO  :    OUTPUT_LARGE_RECORDS: 0
INFO  :    OUTPUT_RECORDS: 720
INFO  :    SPILLED_RECORDS: 0
INFO  : TaskCounter_Map_6_INPUT_store:
INFO  :    INPUT_RECORDS_PROCESSED: 2
INFO  :    INPUT_SPLIT_LENGTH_BYTES: 127422
INFO  : TaskCounter_Map_6_OUTPUT_Map_1:
INFO  :    ADDITIONAL_SPILLS_BYTES_READ: 0
INFO  :    ADDITIONAL_SPILLS_BYTES_WRITTEN: 0
INFO  :    ADDITIONAL_SPILL_COUNT: 0
INFO  :    OUTPUT_BYTES: 905
INFO  :    OUTPUT_BYTES_PHYSICAL: 765
INFO  :    OUTPUT_BYTES_WITH_OVERHEAD: 1273
INFO  :    OUTPUT_LARGE_RECORDS: 0
INFO  :    OUTPUT_RECORDS: 181
INFO  :    SPILLED_RECORDS: 0
INFO  : org.apache.hadoop.hive.llap.counters.LlapIOCounters:
INFO  :    ALLOCATED_BYTES: 39845888
INFO  :    ALLOCATED_USED_BYTES: 20864091
INFO  :    CACHE_HIT_BYTES: 0
INFO  :    CACHE_MISS_BYTES: 12194189
INFO  :    CONSUMER_TIME_NS: 227503430
INFO  :    DECODE_TIME_NS: 224052339
INFO  :    HDFS_TIME_NS: 4332330307
INFO  :    METADATA_CACHE_HIT: 15
INFO  :    METADATA_CACHE_MISS: 21
INFO  :    NUM_DECODED_BATCHES: 349
INFO  :    NUM_VECTOR_BATCHES: 3419
INFO  :    ROWS_EMITTED: 3411808
INFO  :    SELECTED_ROWGROUPS: 349
INFO  :    TOTAL_IO_TIME_NS: 4461447322
ERROR : FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Dag received [DAG_TERMINATE, SERVICE_PLUGIN_ERROR] in RUNNING state.Error reported by TaskScheduler [[2:LLAP]][SERVICE_UNAVAILABLE] No LLAP Daemons are runningVertex killed, vertexName=Reducer 3, vertexId=vertex_1483569684333_0039_1_05, diagnostics=[Vertex received Kill while in RUNNING state., Vertex did not succeed due to DAG_TERMINATED, failedTasks:0 killedTasks:1, Vertex vertex_1483569684333_0039_1_05 [Reducer 3] killed/failed due to:DAG_TERMINATED]Vertex killed, vertexName=Reducer 2, vertexId=vertex_1483569684333_0039_1_04, diagnostics=[Vertex received Kill while in RUNNING state., Vertex did not succeed due to DAG_TERMINATED, failedTasks:0 killedTasks:1, Vertex vertex_1483569684333_0039_1_04 [Reducer 2] killed/failed due to:DAG_TERMINATED]Vertex killed, vertexName=Map 1, vertexId=vertex_1483569684333_0039_1_03, diagnostics=[Vertex received Kill while in RUNNING state., Vertex did not succeed due to DAG_TERMINATED, failedTasks:0 killedTasks:428, Vertex vertex_1483569684333_0039_1_03 [Map 1] killed/failed due to:DAG_TERMINATED]DAG did not succeed due to SERVICE_PLUGIN_ERROR. failedVertices:0 killedVertices:3
INFO  : Completed executing command(queryId=hive_20170105095019_8d837f67-8795-49e4-be85-7188ff537a00); Time taken: 140.366 seconds
Error: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Dag received [DAG_TERMINATE, SERVICE_PLUGIN_ERROR] in RUNNING state.Error reported by TaskScheduler [[2:LLAP]][SERVICE_UNAVAILABLE] No LLAP Daemons are runningVertex killed, vertexName=Reducer 3, vertexId=vertex_1483569684333_0039_1_05, diagnostics=[Vertex received Kill while in RUNNING state., Vertex did not succeed due to DAG_TERMINATED, failedTasks:0 killedTasks:1, Vertex vertex_1483569684333_0039_1_05 [Reducer 3] killed/failed due to:DAG_TERMINATED]Vertex killed, vertexName=Reducer 2, vertexId=vertex_1483569684333_0039_1_04, diagnostics=[Vertex received Kill while in RUNNING state., Vertex did not succeed due to DAG_TERMINATED, failedTasks:0 killedTasks:1, Vertex vertex_1483569684333_0039_1_04 [Reducer 2] killed/failed due to:DAG_TERMINATED]Vertex killed, vertexName=Map 1, vertexId=vertex_1483569684333_0039_1_03, diagnostics=[Vertex received Kill while in RUNNING state., Vertex did not succeed due to DAG_TERMINATED, failedTasks:0 killedTasks:428, Vertex vertex_1483569684333_0039_1_03 [Map 1] killed/failed due to:DAG_TERMINATED]DAG did not succeed due to SERVICE_PLUGIN_ERROR. failedVertices:0 killedVertices:3 (state=08S01,code=2)

Closing: 0: jdbc:hive2://h01hn02:10500/tpcds_bin_partitioned_orc_10000
STOP hdp2.5_llap_10000_query96_sql_2017-01-05-09-50:  Thu Jan 5 09:52:40 CST 2017

 South Results Number of Nodes:  9
 Avg CPU Busy:  4.370  Peak Cpu Avg:  51.325 Count > 90% Busy:  0
 Avg Disk Busy:  5.917  Peak Disk Avg:  31.758 Count > 90% busy 0
 Avg Disk Reads per sec:  6849.976  Avg Write per sec:  528.238
 Avg Net TX:   20943.345  Peak TX Avg:  10746.218
 Avg Net RX:   21785.717  Peak RX Avg:  11120.087
 Mem Utilized:  26.488
South CSV
4.370 | 51.325 | 0 | 5.917 | 31.758 | 0 | 6849.976 | 528.238 | 20943.345 | 10746.218 | 21785.717 | 11120.087 | 26.488
