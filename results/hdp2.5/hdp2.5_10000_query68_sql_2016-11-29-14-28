START hdp2.5_10000_query68_sql_2016-11-29-14-28:  Tue Nov 29 14:28:57 CST 2016
beeline --outputformat=csv2 -u jdbc:hiv2:// -i sample-queries-tpcds/testbench.settings-4g-container-tez --database tpcds_bin_partitioned_orc_10000 -f sample-queries-tpcds/query68.sql
Connecting to jdbc:hive2://h01hn02:10500/tpcds_bin_partitioned_orc_10000
Connected to: Apache Hive (version 2.1.0.2.5.1.0-43)
Driver: Hive JDBC (version 1.2.1000.2.5.1.0-43)
Transaction isolation: TRANSACTION_REPEATABLE_READ
Running init script sample-queries-tpch/testbench.settings-4g-container
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> --set ambari.hive.db.schema.name=hive;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set fs.file.impl.disable.cache=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set fs.hdfs.impl.disable.cache=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.auto.convert.join.noconditionaltask=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.auto.convert.join=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> --set hive.auto.convert.sortmerge.join.noconditionaltask=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.auto.convert.sortmerge.join=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.compactor.abortedtxn.threshold=1000;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.compactor.check.interval=300;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.compactor.delta.num.threshold=10;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.compactor.delta.pct.threshold=0.1f;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.compactor.initiator.on=false;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.compactor.worker.threads=0;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.compactor.worker.timeout=86400;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.compute.query.using.stats=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.enforce.bucketing=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.enforce.sorting=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.enforce.sortmergebucketmapjoin=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.execution.engine=mr;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.limit.pushdown.memory.usage=0.04;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.map.aggr=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.mapjoin.bucket.cache.size=10000;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.mapred.reduce.tasks.speculative.execution=false;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.metastore.cache.pinobjtypes=Table,Database,Type,FieldSchema,Order;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.metastore.client.socket.timeout=60;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.metastore.execute.setugi=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.metastore.warehouse.dir=/apps/hive/warehouse;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.optimize.bucketmapjoin.sortedmerge=false;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.optimize.bucketmapjoin=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.optimize.index.filter=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> --set hive.optimize.mapjoin.mapreduce=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.optimize.reducededuplication.min.reducer=4;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.optimize.reducededuplication=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.orc.splits.include.file.footer=false;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.security.authorization.enabled=false;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.security.metastore.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> --set hive.semantic.analyzer.factory.impl=org.apache.hivealog.cli.HCatSemanticAnalyzerFactory;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.server2.enable.doAs=false;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.server2.tez.default.queues=default;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.server2.tez.initialize.default.sessions=false;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.server2.tez.sessions.per.default.queue=1;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.stats.autogather=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.tez.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.txn.max.open.batch=1000;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.txn.timeout=300;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.vectorized.execution.enabled=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.vectorized.groupby.checkinterval=1024;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.vectorized.groupby.flush.percent=1;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.vectorized.groupby.maxentries=1024;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> 
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- These values need to be tuned appropriately to your cluster. These examples are for reference.
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.tez.container.size=4096;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.tez.java.opts=-Xmx3800m;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.auto.convert.join.noconditionaltask.size=1252698795;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> 
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> 
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> select  c_last_name
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>        ,c_first_name
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>        ,ca_city
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>        ,bought_city
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>        ,ss_ticket_number
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>        ,extended_price
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>        ,extended_tax
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>        ,list_price
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>  from (select ss_ticket_number
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>              ,ss_customer_sk
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>              ,ca_city bought_city
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>              ,sum(ss_ext_sales_price) extended_price 
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>              ,sum(ss_ext_list_price) list_price
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>              ,sum(ss_ext_tax) extended_tax 
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>        from store_sales
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>            ,date_dim
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>            ,store
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>            ,household_demographics
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>            ,customer_address 
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>        where store_sales.ss_sold_date_sk = date_dim.d_date_sk
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>          and store_sales.ss_store_sk = store.s_store_sk  
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>         and store_sales.ss_hdemo_sk = household_demographics.hd_demo_sk
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>         and store_sales.ss_addr_sk = customer_address.ca_address_sk
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>         and date_dim.d_dom between 1 and 2 
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>         and (household_demographics.hd_dep_count = 4 or
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>              household_demographics.hd_vehicle_count= 2)
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>         and date_dim.d_year in (1998,1998+1,1998+2)
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>         and store.s_city in ('Rosedale','Bethlehem')
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>        group by ss_ticket_number
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>                ,ss_customer_sk
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>                ,ss_addr_sk,ca_city) dn
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>       ,customer
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>       ,customer_address current_addr
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>  where dn.ss_customer_sk = customer.c_customer_sk
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>    and customer.c_current_addr_sk = current_addr.ca_address_sk
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>    and current_addr.ca_city <> bought_city
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>  order by c_last_name
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>          ,ss_ticket_number
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>  limit 100;
INFO  : Compiling command(queryId=hive_20161129142859_0d387bcf-ea45-45c8-b3c0-0332f4f2891b): select  c_last_name
       ,c_first_name
       ,ca_city
       ,bought_city
       ,ss_ticket_number
       ,extended_price
       ,extended_tax
       ,list_price
 from (select ss_ticket_number
             ,ss_customer_sk
             ,ca_city bought_city
             ,sum(ss_ext_sales_price) extended_price 
             ,sum(ss_ext_list_price) list_price
             ,sum(ss_ext_tax) extended_tax 
       from store_sales
           ,date_dim
           ,store
           ,household_demographics
           ,customer_address 
       where store_sales.ss_sold_date_sk = date_dim.d_date_sk
         and store_sales.ss_store_sk = store.s_store_sk  
        and store_sales.ss_hdemo_sk = household_demographics.hd_demo_sk
        and store_sales.ss_addr_sk = customer_address.ca_address_sk
        and date_dim.d_dom between 1 and 2 
        and (household_demographics.hd_dep_count = 4 or
             household_demographics.hd_vehicle_count= 2)
        and date_dim.d_year in (1998,1998+1,1998+2)
        and store.s_city in ('Rosedale','Bethlehem')
       group by ss_ticket_number
               ,ss_customer_sk
               ,ss_addr_sk,ca_city) dn
      ,customer
      ,customer_address current_addr
 where dn.ss_customer_sk = customer.c_customer_sk
   and customer.c_current_addr_sk = current_addr.ca_address_sk
   and current_addr.ca_city <> bought_city
 order by c_last_name
         ,ss_ticket_number
 limit 100
INFO  : Semantic Analysis Completed
INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:c_last_name, type:string, comment:null), FieldSchema(name:c_first_name, type:string, comment:null), FieldSchema(name:ca_city, type:string, comment:null), FieldSchema(name:bought_city, type:string, comment:null), FieldSchema(name:ss_ticket_number, type:int, comment:null), FieldSchema(name:extended_price, type:double, comment:null), FieldSchema(name:extended_tax, type:double, comment:null), FieldSchema(name:list_price, type:double, comment:null)], properties:null)
INFO  : Completed compiling command(queryId=hive_20161129142859_0d387bcf-ea45-45c8-b3c0-0332f4f2891b); Time taken: 0.685 seconds
INFO  : Concurrency mode is disabled, not creating a lock manager
INFO  : Executing command(queryId=hive_20161129142859_0d387bcf-ea45-45c8-b3c0-0332f4f2891b): select  c_last_name
       ,c_first_name
       ,ca_city
       ,bought_city
       ,ss_ticket_number
       ,extended_price
       ,extended_tax
       ,list_price
 from (select ss_ticket_number
             ,ss_customer_sk
             ,ca_city bought_city
             ,sum(ss_ext_sales_price) extended_price 
             ,sum(ss_ext_list_price) list_price
             ,sum(ss_ext_tax) extended_tax 
       from store_sales
           ,date_dim
           ,store
           ,household_demographics
           ,customer_address 
       where store_sales.ss_sold_date_sk = date_dim.d_date_sk
         and store_sales.ss_store_sk = store.s_store_sk  
        and store_sales.ss_hdemo_sk = household_demographics.hd_demo_sk
        and store_sales.ss_addr_sk = customer_address.ca_address_sk
        and date_dim.d_dom between 1 and 2 
        and (household_demographics.hd_dep_count = 4 or
             household_demographics.hd_vehicle_count= 2)
        and date_dim.d_year in (1998,1998+1,1998+2)
        and store.s_city in ('Rosedale','Bethlehem')
       group by ss_ticket_number
               ,ss_customer_sk
               ,ss_addr_sk,ca_city) dn
      ,customer
      ,customer_address current_addr
 where dn.ss_customer_sk = customer.c_customer_sk
   and customer.c_current_addr_sk = current_addr.ca_address_sk
   and current_addr.ca_city <> bought_city
 order by c_last_name
         ,ss_ticket_number
 limit 100
INFO  : Query ID = hive_20161129142859_0d387bcf-ea45-45c8-b3c0-0332f4f2891b
INFO  : Total jobs = 1
INFO  : Launching Job 1 out of 1
INFO  : Starting task [Stage-1:MAPRED] in serial mode
INFO  : Session is already open
INFO  : Dag name: select  c_last_name
       ,c_first_na...100(Stage-1)
INFO  : Setting tez.task.scale.memory.reserve-fraction to 0.30000001192092896
INFO  : Setting tez.task.scale.memory.reserve-fraction to 0.30000001192092896
INFO  : Setting tez.task.scale.memory.reserve-fraction to 0.30000001192092896
INFO  : Setting tez.task.scale.memory.reserve-fraction to 0.30000001192092896
INFO  : Tez session was closed. Reopening...
INFO  : Dag submit failed due to TezSession has already shutdown. Application killed by user. stack trace: [org.apache.tez.client.TezClient.waitTillReady(TezClient.java:928), org.apache.tez.client.TezClient.waitTillReady(TezClient.java:897), org.apache.hadoop.hive.ql.exec.tez.TezSessionState.startSessionAndContainers(TezSessionState.java:393), org.apache.hadoop.hive.ql.exec.tez.TezSessionState.openInternal(TezSessionState.java:320), org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager$TezSessionPoolSession.openInternal(TezSessionPoolManager.java:622), org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open(TezSessionState.java:200), org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.reopenSession(TezSessionPoolManager.java:478), org.apache.hadoop.hive.ql.exec.tez.TezTask.submit(TezTask.java:465), org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:175), org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:197), org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100), org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1870), org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1574), org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1326), org.apache.hadoop.hive.ql.Driver.run(Driver.java:1095), org.apache.hadoop.hive.ql.Driver.run(Driver.java:1088), org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:242), org.apache.hive.service.cli.operation.SQLOperation.access$800(SQLOperation.java:91), org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:334), java.security.AccessController.doPrivileged(Native Method), javax.security.auth.Subject.doAs(Subject.java:422), org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724), org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:347), java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511), java.util.concurrent.FutureTask.run(FutureTask.java:266), java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511), java.util.concurrent.FutureTask.run(FutureTask.java:266), java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142), java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617), java.lang.Thread.run(Thread.java:745)] retrying...
INFO  : 

INFO  : Status: Running (Executing on YARN cluster with App id application_1478933081481_0219)

INFO  : Map 1: -/-	Map 10: -/-	Map 11: -/-	Map 12: -/-	Map 7: -/-	Map 8: -/-	Map 9: -/-	Reducer 2: 0/1009	Reducer 3: 0/1009	Reducer 4: 0/1009	Reducer 5: 0/1009	Reducer 6: 0/1	
INFO  : Map 1: -/-	Map 10: -/-	Map 11: -/-	Map 12: -/-	Map 7: 0/1	Map 8: 0/1	Map 9: 0/1	Reducer 2: 0/1009	Reducer 3: 0/1009	Reducer 4: 0/1009	Reducer 5: 0/1009	Reducer 6: 0/1	
INFO  : Map 1: -/-	Map 10: -/-	Map 11: 0/281	Map 12: -/-	Map 7: 0/1	Map 8: 0/1	Map 9: 0/1	Reducer 2: 0/1009	Reducer 3: 0/1009	Reducer 4: 0/1009	Reducer 5: 0/1009	Reducer 6: 0/1	
INFO  : Map 1: -/-	Map 10: -/-	Map 11: 0/281	Map 12: -/-	Map 7: 0/1	Map 8: 0/1	Map 9: 0(+1)/1	Reducer 2: 0/1009	Reducer 3: 0/1009	Reducer 4: 0/1009	Reducer 5: 0/1009	Reducer 6: 0/1	
INFO  : Map 1: -/-	Map 10: -/-	Map 11: 0(+55)/281	Map 12: -/-	Map 7: 0(+1)/1	Map 8: 0(+1)/1	Map 9: 0(+1)/1	Reducer 2: 0/1009	Reducer 3: 0/1009	Reducer 4: 0/1009	Reducer 5: 0/1009	Reducer 6: 0/1	
INFO  : Map 1: -/-	Map 10: -/-	Map 11: 0(+67)/281	Map 12: -/-	Map 7: 0(+1)/1	Map 8: 0(+1)/1	Map 9: 0(+1)/1	Reducer 2: 0/1009	Reducer 3: 0/1009	Reducer 4: 0/1009	Reducer 5: 0/1009	Reducer 6: 0/1	
INFO  : Status: Killed
ERROR : Application killed by user.
ERROR : FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Application killed by user.
INFO  : Completed executing command(queryId=hive_20161129142859_0d387bcf-ea45-45c8-b3c0-0332f4f2891b); Time taken: 31.08 seconds
Error: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Application killed by user. (state=08S01,code=1)

Closing: 0: jdbc:hive2://h01hn02:10500/tpcds_bin_partitioned_orc_10000
STOP hdp2.5_10000_query68_sql_2016-11-29-14-28:  Tue Nov 29 14:29:31 CST 2016

 South Results Number of Nodes:  9
 Avg CPU Busy:   Peak Cpu Avg:  99.965 Count > 90% Busy:  .111
 Avg Disk Busy:   Peak Disk Avg:  11.079 Count > 90% busy 0
 Avg Disk Reads per sec:   Avg Write per sec: 
 Avg Net TX:    Peak TX Avg: 
 Avg Net RX:    Peak RX Avg: 
 Mem Utilized: 
South CSV
| 99.965 | .111 | | 11.079 | 0 | | | | | | |
