START hdp2.5_10000_query96_sql_2016-12-01-12-33:  Thu Dec 1 12:33:54 CST 2016
beeline --outputformat=csv2 -u jdbc:hiv2:// -i sample-queries-tpcds/testbench.settings-4g-container-tez --database tpcds_bin_partitioned_orc_10000 -f sample-queries-tpcds/query96.sql
Connecting to jdbc:hive2://h01hn02:10500/tpcds_bin_partitioned_orc_10000
Connected to: Apache Hive (version 2.1.0.2.5.1.0-43)
Driver: Hive JDBC (version 1.2.1000.2.5.1.0-43)
Transaction isolation: TRANSACTION_REPEATABLE_READ
Running init script sample-queries-tpch/testbench.settings-4g-container
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> --set ambari.hive.db.schema.name=hive;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set fs.file.impl.disable.cache=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set fs.hdfs.impl.disable.cache=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.auto.convert.join.noconditionaltask=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.auto.convert.join=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> --set hive.auto.convert.sortmerge.join.noconditionaltask=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.auto.convert.sortmerge.join=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.compactor.abortedtxn.threshold=1000;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.compactor.check.interval=300;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.compactor.delta.num.threshold=10;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.compactor.delta.pct.threshold=0.1f;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.compactor.initiator.on=false;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.compactor.worker.threads=0;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.compactor.worker.timeout=86400;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.compute.query.using.stats=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.enforce.bucketing=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.enforce.sorting=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.enforce.sortmergebucketmapjoin=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.execution.engine=mr;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.limit.pushdown.memory.usage=0.04;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.map.aggr=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.mapjoin.bucket.cache.size=10000;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.mapred.reduce.tasks.speculative.execution=false;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.metastore.cache.pinobjtypes=Table,Database,Type,FieldSchema,Order;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.metastore.client.socket.timeout=60;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.metastore.execute.setugi=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.metastore.warehouse.dir=/apps/hive/warehouse;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.optimize.bucketmapjoin.sortedmerge=false;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.optimize.bucketmapjoin=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.optimize.index.filter=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> --set hive.optimize.mapjoin.mapreduce=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.optimize.reducededuplication.min.reducer=4;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.optimize.reducededuplication=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.orc.splits.include.file.footer=false;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.security.authorization.enabled=false;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.security.metastore.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.Storag eBasedAuthorizationProvider;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> --set hive.semantic.analyzer.factory.impl=org.apache.hivealog.cli.HCatSemanticAnalyzerFactory;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.server2.enable.doAs=false;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.server2.tez.default.queues=default;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.server2.tez.initialize.default.sessions=false;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.server2.tez.sessions.per.default.queue=1;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.stats.autogather=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.tez.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.txn.max.open.batch=1000;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.txn.timeout=300;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.vectorized.execution.enabled=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.vectorized.groupby.checkinterval=1024;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.vectorized.groupby.flush.percent=1;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.vectorized.groupby.maxentries=1024;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> 
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- These values need to be tuned appropriately to your cluster. These examples are for reference.
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.tez.container.size=4096;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.tez.java.opts=-Xmx3800m;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.auto.convert.join.noconditionaltask.size=1252698795;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> 
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> 
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> select  count(*) as c
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> from store_sales
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>     ,household_demographics 
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>     ,time_dim, store
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> where store_sales.ss_sold_time_sk = time_dim.t_time_sk   
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>     and store_sales.ss_hdemo_sk = household_demographics.hd_demo_sk 
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>     and store_sales.ss_store_sk = store.s_store_sk
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>     and time_dim.t_hour = 8
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>     and time_dim.t_minute >= 30
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>     and household_demographics.hd_dep_count = 5
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>     and store.s_store_name = 'ese'
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> order by c
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> limit 100;
INFO  : Compiling command(queryId=hive_20161201123356_0ead5689-e39c-4d5e-8293-cfb561324419): select  count(*) as c
from store_sales
    ,household_demographics 
    ,time_dim, store
where store_sales.ss_sold_time_sk = time_dim.t_time_sk   
    and store_sales.ss_hdemo_sk = household_demographics.hd_demo_sk 
    and store_sales.ss_store_sk = store.s_store_sk
    and time_dim.t_hour = 8
    and time_dim.t_minute >= 30
    and household_demographics.hd_dep_count = 5
    and store.s_store_name = 'ese'
order by c
limit 100
INFO  : Semantic Analysis Completed
INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:c, type:bigint, comment:null)], properties:null)
INFO  : Completed compiling command(queryId=hive_20161201123356_0ead5689-e39c-4d5e-8293-cfb561324419); Time taken: 0.497 seconds
INFO  : Concurrency mode is disabled, not creating a lock manager
INFO  : Executing command(queryId=hive_20161201123356_0ead5689-e39c-4d5e-8293-cfb561324419): select  count(*) as c
from store_sales
    ,household_demographics 
    ,time_dim, store
where store_sales.ss_sold_time_sk = time_dim.t_time_sk   
    and store_sales.ss_hdemo_sk = household_demographics.hd_demo_sk 
    and store_sales.ss_store_sk = store.s_store_sk
    and time_dim.t_hour = 8
    and time_dim.t_minute >= 30
    and household_demographics.hd_dep_count = 5
    and store.s_store_name = 'ese'
order by c
limit 100
INFO  : Query ID = hive_20161201123356_0ead5689-e39c-4d5e-8293-cfb561324419
INFO  : Total jobs = 1
INFO  : Launching Job 1 out of 1
INFO  : Starting task [Stage-1:MAPRED] in serial mode
INFO  : Session is already open
INFO  : Dag name: select  count(*) as c
from store_sales...100(Stage-1)
INFO  : Setting tez.task.scale.memory.reserve-fraction to 0.30000001192092896
INFO  : Tez session was closed. Reopening...
INFO  : Session re-established.
INFO  : 

INFO  : Status: Running (Executing on YARN cluster with App id application_1478933081481_0250)

INFO  : Map 1: -/-	Map 4: -/-	Map 5: -/-	Map 6: -/-	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: -/-	Map 4: 0/1	Map 5: 0/1	Map 6: 0/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: -/-	Map 4: 0(+1)/1	Map 5: 0(+1)/1	Map 6: 0(+1)/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: -/-	Map 4: 0(+1)/1	Map 5: 1/1	Map 6: 0(+1)/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: -/-	Map 4: 0(+1)/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: -/-	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: -/-	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: -/-	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: -/-	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 0/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 0(+42)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 0(+98)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 0(+260)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 0(+429)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 0(+429)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-14)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-35)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-35)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-35)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-52)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-52)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-52)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-52)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-52)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-52)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-52)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-52)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-52)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-52)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-52)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-52)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-52)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-52)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-52)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-52)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-52)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-52)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-52)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-52)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-52)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-52)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-52)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-52)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-52)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-52)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-52)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-52)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-52)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-52)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-52)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-52)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
INFO  : Map 1: 1(+428,-52)/429	Map 4: 1/1	Map 5: 1/1	Map 6: 1/1	Reducer 2: 0/1	Reducer 3: 0/1	
ERROR : Status: Failed
ERROR : Dag received [DAG_TERMINATE, SERVICE_PLUGIN_ERROR] in RUNNING state.
ERROR : Error reported by TaskScheduler [[2:LLAP]][SERVICE_UNAVAILABLE] No LLAP Daemons are running
ERROR : Vertex killed, vertexName=Reducer 3, vertexId=vertex_1478933081481_0250_1_05, diagnostics=[Vertex received Kill while in RUNNING state., Vertex did not succeed due to DAG_TERMINATED, failedTasks:0 killedTasks:1, Vertex vertex_1478933081481_0250_1_05 [Reducer 3] killed/failed due to:DAG_TERMINATED]
ERROR : Vertex killed, vertexName=Reducer 2, vertexId=vertex_1478933081481_0250_1_04, diagnostics=[Vertex received Kill while in RUNNING state., Vertex did not succeed due to DAG_TERMINATED, failedTasks:0 killedTasks:1, Vertex vertex_1478933081481_0250_1_04 [Reducer 2] killed/failed due to:DAG_TERMINATED]
ERROR : Vertex killed, vertexName=Map 1, vertexId=vertex_1478933081481_0250_1_03, diagnostics=[Vertex received Kill while in RUNNING state., Vertex did not succeed due to DAG_TERMINATED, failedTasks:0 killedTasks:428, Vertex vertex_1478933081481_0250_1_03 [Map 1] killed/failed due to:DAG_TERMINATED]
ERROR : DAG did not succeed due to SERVICE_PLUGIN_ERROR. failedVertices:0 killedVertices:3
INFO  : org.apache.tez.common.counters.DAGCounter:
INFO  :    NUM_FAILED_TASKS: 52
INFO  :    NUM_KILLED_TASKS: 836
INFO  :    NUM_SUCCEEDED_TASKS: 4
INFO  :    TOTAL_LAUNCHED_TASKS: 500
INFO  :    DATA_LOCAL_TASKS: 4
INFO  :    AM_CPU_MILLISECONDS: 133460
INFO  :    AM_GC_TIME_MILLIS: 3205
INFO  : File System Counters:
INFO  :    FILE_BYTES_READ: 64
INFO  :    FILE_BYTES_WRITTEN: 11143
INFO  :    FILE_READ_OPS: 0
INFO  :    FILE_LARGE_READ_OPS: 0
INFO  :    FILE_WRITE_OPS: 0
INFO  :    HDFS_BYTES_READ: 12265817
INFO  :    HDFS_BYTES_WRITTEN: 0
INFO  :    HDFS_READ_OPS: 42
INFO  :    HDFS_LARGE_READ_OPS: 0
INFO  :    HDFS_WRITE_OPS: 0
INFO  : org.apache.tez.common.counters.TaskCounter:
INFO  :    SPILLED_RECORDS: 1
INFO  :    INPUT_RECORDS_PROCESSED: 3419
INFO  :    INPUT_SPLIT_LENGTH_BYTES: 185656419
INFO  :    OUTPUT_RECORDS: 2702
INFO  :    OUTPUT_LARGE_RECORDS: 0
INFO  :    OUTPUT_BYTES: 13510
INFO  :    OUTPUT_BYTES_WITH_OVERHEAD: 18938
INFO  :    OUTPUT_BYTES_PHYSICAL: 11087
INFO  :    ADDITIONAL_SPILLS_BYTES_WRITTEN: 0
INFO  :    ADDITIONAL_SPILLS_BYTES_READ: 0
INFO  :    ADDITIONAL_SPILL_COUNT: 0
INFO  :    SHUFFLE_CHUNK_COUNT: 1
INFO  :    SHUFFLE_BYTES_DECOMPRESSED: 0
INFO  : HIVE:
INFO  :    DESERIALIZE_ERRORS: 0
INFO  :    RECORDS_IN_Map_1: 3383108
INFO  :    RECORDS_IN_Map_4: 7200
INFO  :    RECORDS_IN_Map_5: 20000
INFO  :    RECORDS_IN_Map_6: 1500
INFO  :    RECORDS_OUT_INTERMEDIATE_Map_1: 1
INFO  :    RECORDS_OUT_INTERMEDIATE_Map_4: 720
INFO  :    RECORDS_OUT_INTERMEDIATE_Map_5: 1800
INFO  :    RECORDS_OUT_INTERMEDIATE_Map_6: 181
INFO  : TaskCounter_Map_1_INPUT_Map_4:
INFO  :    INPUT_RECORDS_PROCESSED: 0
INFO  :    SHUFFLE_BYTES_DECOMPRESSED: 0
INFO  : TaskCounter_Map_1_INPUT_Map_5:
INFO  :    INPUT_RECORDS_PROCESSED: 0
INFO  :    SHUFFLE_BYTES_DECOMPRESSED: 0
INFO  : TaskCounter_Map_1_INPUT_Map_6:
INFO  :    INPUT_RECORDS_PROCESSED: 0
INFO  :    SHUFFLE_BYTES_DECOMPRESSED: 0
INFO  : TaskCounter_Map_1_INPUT_store_sales:
INFO  :    INPUT_RECORDS_PROCESSED: 3389
INFO  :    INPUT_SPLIT_LENGTH_BYTES: 185079271
INFO  : TaskCounter_Map_1_OUTPUT_Reducer_2:
INFO  :    ADDITIONAL_SPILLS_BYTES_READ: 0
INFO  :    ADDITIONAL_SPILLS_BYTES_WRITTEN: 0
INFO  :    ADDITIONAL_SPILL_COUNT: 0
INFO  :    OUTPUT_BYTES: 5
INFO  :    OUTPUT_BYTES_PHYSICAL: 27
INFO  :    OUTPUT_BYTES_WITH_OVERHEAD: 13
INFO  :    OUTPUT_RECORDS: 1
INFO  :    SHUFFLE_CHUNK_COUNT: 1
INFO  :    SPILLED_RECORDS: 1
INFO  : TaskCounter_Map_4_INPUT_household_demographics:
INFO  :    INPUT_RECORDS_PROCESSED: 8
INFO  :    INPUT_SPLIT_LENGTH_BYTES: 1147
INFO  : TaskCounter_Map_4_OUTPUT_Map_1:
INFO  :    ADDITIONAL_SPILLS_BYTES_READ: 0
INFO  :    ADDITIONAL_SPILLS_BYTES_WRITTEN: 0
INFO  :    ADDITIONAL_SPILL_COUNT: 0
INFO  :    OUTPUT_BYTES: 3600
INFO  :    OUTPUT_BYTES_PHYSICAL: 3073
INFO  :    OUTPUT_BYTES_WITH_OVERHEAD: 5046
INFO  :    OUTPUT_LARGE_RECORDS: 0
INFO  :    OUTPUT_RECORDS: 720
INFO  :    SPILLED_RECORDS: 0
INFO  : TaskCounter_Map_5_INPUT_time_dim:
INFO  :    INPUT_RECORDS_PROCESSED: 20
INFO  :    INPUT_SPLIT_LENGTH_BYTES: 448579
INFO  : TaskCounter_Map_5_OUTPUT_Map_1:
INFO  :    ADDITIONAL_SPILLS_BYTES_READ: 0
INFO  :    ADDITIONAL_SPILLS_BYTES_WRITTEN: 0
INFO  :    ADDITIONAL_SPILL_COUNT: 0
INFO  :    OUTPUT_BYTES: 9000
INFO  :    OUTPUT_BYTES_PHYSICAL: 7222
INFO  :    OUTPUT_BYTES_WITH_OVERHEAD: 12606
INFO  :    OUTPUT_LARGE_RECORDS: 0
INFO  :    OUTPUT_RECORDS: 1800
INFO  :    SPILLED_RECORDS: 0
INFO  : TaskCounter_Map_6_INPUT_store:
INFO  :    INPUT_RECORDS_PROCESSED: 2
INFO  :    INPUT_SPLIT_LENGTH_BYTES: 127422
INFO  : TaskCounter_Map_6_OUTPUT_Map_1:
INFO  :    ADDITIONAL_SPILLS_BYTES_READ: 0
INFO  :    ADDITIONAL_SPILLS_BYTES_WRITTEN: 0
INFO  :    ADDITIONAL_SPILL_COUNT: 0
INFO  :    OUTPUT_BYTES: 905
INFO  :    OUTPUT_BYTES_PHYSICAL: 765
INFO  :    OUTPUT_BYTES_WITH_OVERHEAD: 1273
INFO  :    OUTPUT_LARGE_RECORDS: 0
INFO  :    OUTPUT_RECORDS: 181
INFO  :    SPILLED_RECORDS: 0
INFO  : org.apache.hadoop.hive.llap.counters.LlapIOCounters:
INFO  :    ALLOCATED_BYTES: 39845888
INFO  :    ALLOCATED_USED_BYTES: 20864091
INFO  :    CACHE_HIT_BYTES: 0
INFO  :    CACHE_MISS_BYTES: 12194189
INFO  :    CONSUMER_TIME_NS: 225648793
INFO  :    DECODE_TIME_NS: 222100189
INFO  :    HDFS_TIME_NS: 5280659633
INFO  :    METADATA_CACHE_HIT: 15
INFO  :    METADATA_CACHE_MISS: 21
INFO  :    NUM_DECODED_BATCHES: 349
INFO  :    NUM_VECTOR_BATCHES: 3419
INFO  :    ROWS_EMITTED: 3411808
INFO  :    SELECTED_ROWGROUPS: 349
INFO  :    TOTAL_IO_TIME_NS: 5420406441
ERROR : FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Dag received [DAG_TERMINATE, SERVICE_PLUGIN_ERROR] in RUNNING state.Error reported by TaskScheduler [[2:LLAP]][SERVICE_UNAVAILABLE] No LLAP Daemons are runningVertex killed, vertexName=Reducer 3, vertexId=vertex_1478933081481_0250_1_05, diagnostics=[Vertex received Kill while in RUNNING state., Vertex did not succeed due to DAG_TERMINATED, failedTasks:0 killedTasks:1, Vertex vertex_1478933081481_0250_1_05 [Reducer 3] killed/failed due to:DAG_TERMINATED]Vertex killed, vertexName=Reducer 2, vertexId=vertex_1478933081481_0250_1_04, diagnostics=[Vertex received Kill while in RUNNING state., Vertex did not succeed due to DAG_TERMINATED, failedTasks:0 killedTasks:1, Vertex vertex_1478933081481_0250_1_04 [Reducer 2] killed/failed due to:DAG_TERMINATED]Vertex killed, vertexName=Map 1, vertexId=vertex_1478933081481_0250_1_03, diagnostics=[Vertex received Kill while in RUNNING state., Vertex did not succeed due to DAG_TERMINATED, failedTasks:0 killedTasks:428, Vertex vertex_1478933081481_0250_1_03 [Map 1] killed/failed due to:DAG_TERMINATED]DAG did not succeed due to SERVICE_PLUGIN_ERROR. failedVertices:0 killedVertices:3
INFO  : Completed executing command(queryId=hive_20161201123356_0ead5689-e39c-4d5e-8293-cfb561324419); Time taken: 137.533 seconds
Error: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Dag received [DAG_TERMINATE, SERVICE_PLUGIN_ERROR] in RUNNING state.Error reported by TaskScheduler [[2:LLAP]][SERVICE_UNAVAILABLE] No LLAP Daemons are runningVertex killed, vertexName=Reducer 3, vertexId=vertex_1478933081481_0250_1_05, diagnostics=[Vertex received Kill while in RUNNING state., Vertex did not succeed due to DAG_TERMINATED, failedTasks:0 killedTasks:1, Vertex vertex_1478933081481_0250_1_05 [Reducer 3] killed/failed due to:DAG_TERMINATED]Vertex killed, vertexName=Reducer 2, vertexId=vertex_1478933081481_0250_1_04, diagnostics=[Vertex received Kill while in RUNNING state., Vertex did not succeed due to DAG_TERMINATED, failedTasks:0 killedTasks:1, Vertex vertex_1478933081481_0250_1_04 [Reducer 2] killed/failed due to:DAG_TERMINATED]Vertex killed, vertexName=Map 1, vertexId=vertex_1478933081481_0250_1_03, diagnostics=[Vertex received Kill while in RUNNING state., Vertex did not succeed due to DAG_TERMINATED, failedTasks:0 killedTasks:428, Vertex vertex_1478933081481_0250_1_03 [Map 1] killed/failed due to:DAG_TERMINATED]DAG did not succeed due to SERVICE_PLUGIN_ERROR. failedVertices:0 killedVertices:3 (state=08S01,code=2)

Closing: 0: jdbc:hive2://h01hn02:10500/tpcds_bin_partitioned_orc_10000
STOP hdp2.5_10000_query96_sql_2016-12-01-12-33:  Thu Dec 1 12:36:14 CST 2016

 South Results Number of Nodes:  9
 Avg CPU Busy:  5.250  Peak Cpu Avg:  50.886 Count > 90% Busy:  0
 Avg Disk Busy:  7.538  Peak Disk Avg:  40.589 Count > 90% busy 0
 Avg Disk Reads per sec:  8766.062  Avg Write per sec:  222.107
 Avg Net TX:   21903.080  Peak TX Avg:  11334.487
 Avg Net RX:   22127.358  Peak RX Avg:  11273.587
 Mem Utilized:  31.653
South CSV
5.250 | 50.886 | 0 | 7.538 | 40.589 | 0 | 8766.062 | 222.107 | 21903.080 | 11334.487 | 22127.358 | 11273.587 | 31.653
