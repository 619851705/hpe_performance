START hdp2.5_10000_query67_sql_2016-11-29-14-27:  Tue Nov 29 14:27:45 CST 2016
beeline --outputformat=csv2 -u jdbc:hiv2:// -i sample-queries-tpcds/testbench.settings-4g-container-tez --database tpcds_bin_partitioned_orc_10000 -f sample-queries-tpcds/query67.sql
Connecting to jdbc:hive2://h01hn02:10500/tpcds_bin_partitioned_orc_10000
Connected to: Apache Hive (version 2.1.0.2.5.1.0-43)
Driver: Hive JDBC (version 1.2.1000.2.5.1.0-43)
Transaction isolation: TRANSACTION_REPEATABLE_READ
Running init script sample-queries-tpch/testbench.settings-4g-container
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> --set ambari.hive.db.schema.name=hive;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set fs.file.impl.disable.cache=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set fs.hdfs.impl.disable.cache=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.auto.convert.join.noconditionaltask=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.auto.convert.join=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> --set hive.auto.convert.sortmerge.join.noconditionaltask=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.auto.convert.sortmerge.join=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.compactor.abortedtxn.threshold=1000;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.compactor.check.interval=300;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.compactor.delta.num.threshold=10;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.compactor.delta.pct.threshold=0.1f;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.compactor.initiator.on=false;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.compactor.worker.threads=0;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.compactor.worker.timeout=86400;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.compute.query.using.stats=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.enforce.bucketing=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.enforce.sorting=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.enforce.sortmergebucketmapjoin=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.execution.engine=mr;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.limit.pushdown.memory.usage=0.04;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.map.aggr=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.mapjoin.bucket.cache.size=10000;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.mapred.reduce.tasks.speculative.execution=false;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.metastore.cache.pinobjtypes=Table,Database,Type,FieldSchema,Order;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.metastore.client.socket.timeout=60;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.metastore.execute.setugi=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.metastore.warehouse.dir=/apps/hive/warehouse;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.optimize.bucketmapjoin.sortedmerge=false;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.optimize.bucketmapjoin=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.optimize.index.filter=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> --set hive.optimize.mapjoin.mapreduce=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.optimize.reducededuplication.min.reducer=4;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.optimize.reducededuplication=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.orc.splits.include.file.footer=false;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.security.authorization.enabled=false;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.security.metastore.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> --set hive.semantic.analyzer.factory.impl=org.apache.hivealog.cli.HCatSemanticAnalyzerFactory;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.server2.enable.doAs=false;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.server2.tez.default.queues=default;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.server2.tez.initialize.default.sessions=false;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.server2.tez.sessions.per.default.queue=1;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.stats.autogather=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.tez.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.txn.max.open.batch=1000;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.txn.timeout=300;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.vectorized.execution.enabled=true;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.vectorized.groupby.checkinterval=1024;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.vectorized.groupby.flush.percent=1;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.vectorized.groupby.maxentries=1024;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> 
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- These values need to be tuned appropriately to your cluster. These examples are for reference.
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.tez.container.size=4096;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.tez.java.opts=-Xmx3800m;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> -- set hive.auto.convert.join.noconditionaltask.size=1252698795;
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> 
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> 
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> select  *
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> from (select i_category
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>             ,i_class
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>             ,i_brand
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>             ,i_product_name
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>             ,d_year
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>             ,d_qoy
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>             ,d_moy
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>             ,s_store_id
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>             ,sumsales
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>             ,rank() over (partition by i_category order by sumsales desc) rk
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>       from (select i_category
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>                   ,i_class
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>                   ,i_brand
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>                   ,i_product_name
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>                   ,d_year
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>                   ,d_qoy
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>                   ,d_moy
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>                   ,s_store_id
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>                   ,sum(coalesce(ss_sales_price*ss_quantity,0)) sumsales
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>             from store_sales
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>                 ,date_dim
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>                 ,store
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>                 ,item
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>        where  store_sales.ss_sold_date_sk=date_dim.d_date_sk
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>           and store_sales.ss_item_sk=item.i_item_sk
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>           and store_sales.ss_store_sk = store.s_store_sk
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>           and d_month_seq between 1193 and 1193+11
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>        group by i_category, i_class, i_brand, i_product_name, d_year, d_qoy, d_moy,s_store_id with rollup)dw1) dw2
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> where rk <= 100
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> order by i_category
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>         ,i_class
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>         ,i_brand
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>         ,i_product_name
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>         ,d_year
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>         ,d_qoy
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>         ,d_moy
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>         ,s_store_id
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>         ,sumsales
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti>         ,rk
0: jdbc:hive2://h01hn02:10500/tpcds_bin_parti> limit 100;
INFO  : Compiling command(queryId=hive_20161129142746_5db95d8a-d349-42a0-812e-c20b656947aa): select  *
from (select i_category
            ,i_class
            ,i_brand
            ,i_product_name
            ,d_year
            ,d_qoy
            ,d_moy
            ,s_store_id
            ,sumsales
            ,rank() over (partition by i_category order by sumsales desc) rk
      from (select i_category
                  ,i_class
                  ,i_brand
                  ,i_product_name
                  ,d_year
                  ,d_qoy
                  ,d_moy
                  ,s_store_id
                  ,sum(coalesce(ss_sales_price*ss_quantity,0)) sumsales
            from store_sales
                ,date_dim
                ,store
                ,item
       where  store_sales.ss_sold_date_sk=date_dim.d_date_sk
          and store_sales.ss_item_sk=item.i_item_sk
          and store_sales.ss_store_sk = store.s_store_sk
          and d_month_seq between 1193 and 1193+11
       group by i_category, i_class, i_brand, i_product_name, d_year, d_qoy, d_moy,s_store_id with rollup)dw1) dw2
where rk <= 100
order by i_category
        ,i_class
        ,i_brand
        ,i_product_name
        ,d_year
        ,d_qoy
        ,d_moy
        ,s_store_id
        ,sumsales
        ,rk
limit 100
INFO  : Semantic Analysis Completed
INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:dw2.i_category, type:string, comment:null), FieldSchema(name:dw2.i_class, type:string, comment:null), FieldSchema(name:dw2.i_brand, type:string, comment:null), FieldSchema(name:dw2.i_product_name, type:string, comment:null), FieldSchema(name:dw2.d_year, type:int, comment:null), FieldSchema(name:dw2.d_qoy, type:int, comment:null), FieldSchema(name:dw2.d_moy, type:int, comment:null), FieldSchema(name:dw2.s_store_id, type:string, comment:null), FieldSchema(name:dw2.sumsales, type:double, comment:null), FieldSchema(name:dw2.rk, type:int, comment:null)], properties:null)
INFO  : Completed compiling command(queryId=hive_20161129142746_5db95d8a-d349-42a0-812e-c20b656947aa); Time taken: 0.562 seconds
INFO  : Concurrency mode is disabled, not creating a lock manager
INFO  : Executing command(queryId=hive_20161129142746_5db95d8a-d349-42a0-812e-c20b656947aa): select  *
from (select i_category
            ,i_class
            ,i_brand
            ,i_product_name
            ,d_year
            ,d_qoy
            ,d_moy
            ,s_store_id
            ,sumsales
            ,rank() over (partition by i_category order by sumsales desc) rk
      from (select i_category
                  ,i_class
                  ,i_brand
                  ,i_product_name
                  ,d_year
                  ,d_qoy
                  ,d_moy
                  ,s_store_id
                  ,sum(coalesce(ss_sales_price*ss_quantity,0)) sumsales
            from store_sales
                ,date_dim
                ,store
                ,item
       where  store_sales.ss_sold_date_sk=date_dim.d_date_sk
          and store_sales.ss_item_sk=item.i_item_sk
          and store_sales.ss_store_sk = store.s_store_sk
          and d_month_seq between 1193 and 1193+11
       group by i_category, i_class, i_brand, i_product_name, d_year, d_qoy, d_moy,s_store_id with rollup)dw1) dw2
where rk <= 100
order by i_category
        ,i_class
        ,i_brand
        ,i_product_name
        ,d_year
        ,d_qoy
        ,d_moy
        ,s_store_id
        ,sumsales
        ,rk
limit 100
INFO  : Query ID = hive_20161129142746_5db95d8a-d349-42a0-812e-c20b656947aa
INFO  : Total jobs = 1
INFO  : Launching Job 1 out of 1
INFO  : Starting task [Stage-1:MAPRED] in serial mode
INFO  : Session is already open
INFO  : Dag name: select  *
from (select i_category
    ...100(Stage-1)
INFO  : Setting tez.task.scale.memory.reserve-fraction to 0.30000001192092896
INFO  : Tez session was closed. Reopening...
INFO  : Dag submit failed due to TezSession has already shutdown. Application killed by user. stack trace: [org.apache.tez.client.TezClient.waitTillReady(TezClient.java:928), org.apache.tez.client.TezClient.waitTillReady(TezClient.java:897), org.apache.hadoop.hive.ql.exec.tez.TezSessionState.startSessionAndContainers(TezSessionState.java:393), org.apache.hadoop.hive.ql.exec.tez.TezSessionState.openInternal(TezSessionState.java:320), org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager$TezSessionPoolSession.openInternal(TezSessionPoolManager.java:622), org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open(TezSessionState.java:200), org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.reopenSession(TezSessionPoolManager.java:478), org.apache.hadoop.hive.ql.exec.tez.TezTask.submit(TezTask.java:465), org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:175), org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:197), org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100), org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1870), org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1574), org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1326), org.apache.hadoop.hive.ql.Driver.run(Driver.java:1095), org.apache.hadoop.hive.ql.Driver.run(Driver.java:1088), org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:242), org.apache.hive.service.cli.operation.SQLOperation.access$800(SQLOperation.java:91), org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:334), java.security.AccessController.doPrivileged(Native Method), javax.security.auth.Subject.doAs(Subject.java:422), org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724), org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:347), java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511), java.util.concurrent.FutureTask.run(FutureTask.java:266), java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511), java.util.concurrent.FutureTask.run(FutureTask.java:266), java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142), java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617), java.lang.Thread.run(Thread.java:745)] retrying...
INFO  : 

INFO  : Status: Running (Executing on YARN cluster with App id application_1478933081481_0217)

INFO  : Map 1: -/-	Map 5: -/-	Map 6: -/-	Map 7: -/-	Reducer 2: 0/1009	Reducer 3: 0/1009	Reducer 4: 0/1	
INFO  : Map 1: -/-	Map 5: 0/1	Map 6: 0/1	Map 7: 0/7	Reducer 2: 0/1009	Reducer 3: 0/1009	Reducer 4: 0/1	
INFO  : Map 1: -/-	Map 5: 0/1	Map 6: 0(+1)/1	Map 7: 0/7	Reducer 2: 0/1009	Reducer 3: 0/1009	Reducer 4: 0/1	
INFO  : Map 1: -/-	Map 5: 0(+1)/1	Map 6: 0(+1)/1	Map 7: 0(+7)/7	Reducer 2: 0/1009	Reducer 3: 0/1009	Reducer 4: 0/1	
INFO  : Status: Killed
ERROR : Application killed by user.
ERROR : FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Application killed by user.
INFO  : Completed executing command(queryId=hive_20161129142746_5db95d8a-d349-42a0-812e-c20b656947aa); Time taken: 46.734 seconds
Error: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Application killed by user. (state=08S01,code=1)

Closing: 0: jdbc:hive2://h01hn02:10500/tpcds_bin_partitioned_orc_10000
STOP hdp2.5_10000_query67_sql_2016-11-29-14-27:  Tue Nov 29 14:28:33 CST 2016

 South Results Number of Nodes:  9
 Avg CPU Busy:   Peak Cpu Avg:  99.937 Count > 90% Busy:  .333
 Avg Disk Busy:   Peak Disk Avg:  21.956 Count > 90% busy 0
 Avg Disk Reads per sec:   Avg Write per sec: 
 Avg Net TX:    Peak TX Avg: 
 Avg Net RX:    Peak RX Avg: 
 Mem Utilized: 
South CSV
| 99.937 | .333 | | 21.956 | 0 | | | | | | |
