START hdp2.6_hivetez_run_orc_5000_1_query83_sql_2017-04-19-08-57:  Wed Apr 19 08:57:13 CDT 2017
beeline -u jdbc:hive2://h01mgt.hadoop:2181,h01hn01.hadoop:2181,h01hn02.hadoop:2181/tpcds_bin_partitioned_orc_5000;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2 -n hive --incremental=true -i settings/hive8g-128m.settings -f sample-queries-tpcds/query83.sql
Connecting to jdbc:hive2://h01mgt.hadoop:2181,h01hn01.hadoop:2181,h01hn02.hadoop:2181/tpcds_bin_partitioned_orc_5000;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2
Connected to: Apache Hive (version 1.2.1000.2.6.0.3-8)
Driver: Hive JDBC (version 1.2.1000.2.6.0.3-8)
Transaction isolation: TRANSACTION_REPEATABLE_READ
Running init script settings/hive8g-128m.settings
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set tez.container.max.java.heap.fraction=0.8;
No rows affected (0.041 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> --set hive.log.level=WARN;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.tez.exec.print.summary=true;
No rows affected (0.002 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> --set hive.tez.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.tez.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.enforce.bucketing=true;
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.enforce.sorting=true;
No rows affected (0.002 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.tez.container.size=8192; 
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set tez.am.resource.memory.mb=8192;
No rows affected (0.002 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set tez.task.resource.memory.mb=8192; 
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set tez.runtime.io.sort.mb=3277; 
No rows affected (0.002 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.auto.convert.join.noconditionaltask=true; 
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.auto.convert.join.noconditionaltask.size=2834678415; 
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set tez.runtime.unordered.output.buffer.size-mb=819; 
No rows affected (0.002 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set tez.grouping.min-size=1073741824; 
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set tez.grouping.max-size=2147483648; 
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.exec.parallel=true; 
No rows affected (0.002 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.exec.parallel.thread.number=16; 
No rows affected (0.002 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.tez.auto.reducer.parallelism=true; 
No rows affected (0.002 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.exec.reducers.bytes.per.reducer=134217728; 
No rows affected (0.002 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.optimize.reducededuplication.min.reducer = 1 ;
No rows affected (0.002 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> --set tez.queue.name=llap;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.exec.dynamic.partition.mode=nonstrict;
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.exec.max.dynamic.partitions.pernode=100000;
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.exec.max.dynamic.partitions=100000;
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.exec.max.created.files=1000000;
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.exec.parallel=true;
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> --set hive.exec.reducers.max=2000;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.stats.autogather=true;
No rows affected (0.002 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.optimize.sort.dynamic.partition=true;
No rows affected (0.002 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.stats.autogather=true;
No rows affected (0.011 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.execution.engine=tez;
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.cbo.enable=true;
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.compute.query.using.stats=true;
No rows affected (0.002 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.stats.fetch.column.stats=true;
No rows affected (0.002 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.stats.fetch.partition.stats=true;
No rows affected (0.002 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.vectorized.execution.enabled=true;
No rows affected (0.002 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.vectorized.execution.reduce.enabled = true;
No rows affected (0.002 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.vectorized.execution.reduce.groupby.enabled = true;
No rows affected (0.002 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set mapred.job.reduce.input.buffer.percent=0.0;
No rows affected (0.002 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> --set mapreduce.input.fileinputformat.split.minsize=1073741824;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> --set mapreduce.input.fileinputformat.split.maxsize=2147483648;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> --set mapreduce.input.fileinputformat.split.minsize.per.node=1073741824;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> --set mapreduce.input.fileinputformat.split.minsize.per.rack=1073741824;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> --set hive.tez.java.opts=-XX:+PrintGCDetails -verbose:gc -XX:+PrintGCTimeStamps -XX:+UseNUMA  -XX:+UseG1GC -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set tez.runtime.empty.partitions.info-via-events.enabled=true;
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set tez.runtime.report.partition.stats=true;
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> --set mapred.map.tasks=6;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.limit.pushdown.memory.usage=0.04;
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.map.aggr=true;
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.mapjoin.bucket.cache.size=10000;
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.mapred.reduce.tasks.speculative.execution=false;
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.metastore.cache.pinobjtypes=Table,Database,Type,FieldSchema,Order;
No rows affected (0.002 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.metastore.client.socket.timeout=60;
No rows affected (0.002 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.metastore.execute.setugi=true;
No rows affected (0.002 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> with sr_items as
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>  (select i_item_id item_id,
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>         sum(sr_return_quantity) sr_item_qty
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>  from store_returns,
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>       item,
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>       date_dim
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>  where sr_item_sk = i_item_sk
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>  and   d_date    in 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>         (select d_date
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>         from date_dim
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>         where d_week_seq in 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>                 (select d_week_seq
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>                 from date_dim
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>           where d_date in ('1998-01-02','1998-10-15','1998-11-10')))
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>  and   sr_returned_date_sk   = d_date_sk
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>  group by i_item_id),
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>  cr_items as
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>  (select i_item_id item_id,
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>         sum(cr_return_quantity) cr_item_qty
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>  from catalog_returns,
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>       item,
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>       date_dim
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>  where cr_item_sk = i_item_sk
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>  and   d_date    in 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>         (select d_date
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>         from date_dim
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>         where d_week_seq in 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>                 (select d_week_seq
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>                 from date_dim
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>           where d_date in ('1998-01-02','1998-10-15','1998-11-10')))
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>  and   cr_returned_date_sk   = d_date_sk
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>  group by i_item_id),
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>  wr_items as
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>  (select i_item_id item_id,
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>         sum(wr_return_quantity) wr_item_qty
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>  from web_returns,
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>       item,
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>       date_dim
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>  where wr_item_sk = i_item_sk
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>  and   d_date    in 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>         (select d_date
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>         from date_dim
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>         where d_week_seq in 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>                 (select d_week_seq
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>                 from date_dim
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>                 where d_date in ('1998-01-02','1998-10-15','1998-11-10')))
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>  and   wr_returned_date_sk   = d_date_sk
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>  group by i_item_id)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>   select  sr_items.item_id
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>        ,sr_item_qty
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>        ,sr_item_qty/(sr_item_qty+cr_item_qty+wr_item_qty)/3.0 * 100 sr_dev
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>        ,cr_item_qty
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>        ,cr_item_qty/(sr_item_qty+cr_item_qty+wr_item_qty)/3.0 * 100 cr_dev
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>        ,wr_item_qty
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>        ,wr_item_qty/(sr_item_qty+cr_item_qty+wr_item_qty)/3.0 * 100 wr_dev
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>        ,(sr_item_qty+cr_item_qty+wr_item_qty)/3.0 average
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>  from sr_items
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>      ,cr_items
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>      ,wr_items
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>  where sr_items.item_id=cr_items.item_id
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>    and sr_items.item_id=wr_items.item_id 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>  order by sr_items.item_id
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>          ,sr_item_qty
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>  limit 100;
Error: Error while compiling statement: FAILED: SemanticException [Error 10249]: Line 12:24 Unsupported SubQuery Expression 'd_week_seq': SubQuery cannot use the table alias: date_dim; this is also an alias in the Outer Query and SubQuery contains a unqualified column reference (state=42000,code=10249)

Closing: 0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.hadoop:2181,h01hn02.hadoop:2181/tpcds_bin_partitioned_orc_5000;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2
STOP hdp2.6_hivetez_run_orc_5000_1_query83_sql_2017-04-19-08-57:  Wed Apr 19 08:57:17 CDT 2017
