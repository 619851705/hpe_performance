START hdp2.6_hivetez_run_orc_5000_2_query24_sql_2017-04-18-22-56:  Tue Apr 18 22:56:07 CDT 2017
beeline -u jdbc:hive2://h01mgt.hadoop:2181,h01hn01.hadoop:2181,h01hn02.hadoop:2181/tpcds_bin_partitioned_orc_5000;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2 -n hive --incremental=true -i settings/hive8g-128m.settings -f sample-queries-tpcds/query24.sql
Connecting to jdbc:hive2://h01mgt.hadoop:2181,h01hn01.hadoop:2181,h01hn02.hadoop:2181/tpcds_bin_partitioned_orc_5000;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2
Connected to: Apache Hive (version 1.2.1000.2.6.0.3-8)
Driver: Hive JDBC (version 1.2.1000.2.6.0.3-8)
Transaction isolation: TRANSACTION_REPEATABLE_READ
Running init script settings/hive8g-128m.settings
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set tez.container.max.java.heap.fraction=0.8;
No rows affected (0.043 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> --set hive.log.level=WARN;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.tez.exec.print.summary=true;
No rows affected (0.002 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> --set hive.tez.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.tez.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.enforce.bucketing=true;
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.enforce.sorting=true;
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.tez.container.size=8192; 
No rows affected (0.002 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set tez.am.resource.memory.mb=8192;
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set tez.task.resource.memory.mb=8192; 
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set tez.runtime.io.sort.mb=3277; 
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.auto.convert.join.noconditionaltask=true; 
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.auto.convert.join.noconditionaltask.size=2834678415; 
No rows affected (0.002 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set tez.runtime.unordered.output.buffer.size-mb=819; 
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set tez.grouping.min-size=1073741824; 
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set tez.grouping.max-size=2147483648; 
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.exec.parallel=true; 
No rows affected (0.002 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.exec.parallel.thread.number=16; 
No rows affected (0.002 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.tez.auto.reducer.parallelism=true; 
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.exec.reducers.bytes.per.reducer=134217728; 
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.optimize.reducededuplication.min.reducer = 1 ;
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> --set tez.queue.name=llap;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.exec.dynamic.partition.mode=nonstrict;
No rows affected (0.002 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.exec.max.dynamic.partitions.pernode=100000;
No rows affected (0.002 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.exec.max.dynamic.partitions=100000;
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.exec.max.created.files=1000000;
No rows affected (0.002 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.exec.parallel=true;
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> --set hive.exec.reducers.max=2000;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.stats.autogather=true;
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.optimize.sort.dynamic.partition=true;
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.stats.autogather=true;
No rows affected (0.012 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.execution.engine=tez;
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.cbo.enable=true;
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.compute.query.using.stats=true;
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.stats.fetch.column.stats=true;
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.stats.fetch.partition.stats=true;
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.vectorized.execution.enabled=true;
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.vectorized.execution.reduce.enabled = true;
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.vectorized.execution.reduce.groupby.enabled = true;
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set mapred.job.reduce.input.buffer.percent=0.0;
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> --set mapreduce.input.fileinputformat.split.minsize=1073741824;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> --set mapreduce.input.fileinputformat.split.maxsize=2147483648;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> --set mapreduce.input.fileinputformat.split.minsize.per.node=1073741824;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> --set mapreduce.input.fileinputformat.split.minsize.per.rack=1073741824;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> --set hive.tez.java.opts=-XX:+PrintGCDetails -verbose:gc -XX:+PrintGCTimeStamps -XX:+UseNUMA  -XX:+UseG1GC -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set tez.runtime.empty.partitions.info-via-events.enabled=true;
No rows affected (0.002 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set tez.runtime.report.partition.stats=true;
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> --set mapred.map.tasks=6;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.limit.pushdown.memory.usage=0.04;
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.map.aggr=true;
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.mapjoin.bucket.cache.size=10000;
No rows affected (0.002 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.mapred.reduce.tasks.speculative.execution=false;
No rows affected (0.002 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.metastore.cache.pinobjtypes=Table,Database,Type,FieldSchema,Order;
No rows affected (0.002 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.metastore.client.socket.timeout=60;
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.metastore.execute.setugi=true;
No rows affected (0.001 seconds)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> with ssales as
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> (select c_last_name
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>       ,c_first_name
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>       ,s_store_name
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>       ,ca_state
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>       ,s_state
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>       ,i_color
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>       ,i_current_price
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>       ,i_manager_id
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>       ,i_units
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>       ,i_size
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>       ,sum(ss_sales_price) netpaid
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> from store_sales
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>     ,store_returns
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>     ,store
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>     ,item
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>     ,customer
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>     ,customer_address
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> where ss_ticket_number = sr_ticket_number
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>   and ss_item_sk = sr_item_sk
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>   and ss_customer_sk = c_customer_sk
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>   and ss_item_sk = i_item_sk
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>   and ss_store_sk = s_store_sk
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>   and c_birth_country = upper(ca_country)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>   and s_zip = ca_zip
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> and s_market_id=7
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> group by c_last_name
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>         ,c_first_name
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>         ,s_store_name
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>         ,ca_state
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>         ,s_state
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>         ,i_color
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>         ,i_current_price
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>         ,i_manager_id
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>         ,i_units
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>         ,i_size)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> select c_last_name
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>       ,c_first_name
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>       ,s_store_name
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>       ,sum(netpaid) paid
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> from ssales
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> where i_color = 'orchid'
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> group by c_last_name
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>         ,c_first_name
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>         ,s_store_name
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> having sum(netpaid) > (select 0.05*avg(netpaid)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>                                  from ssales)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> ;
Error: Error while compiling statement: FAILED: ParseException line 46:23 cannot recognize input near 'select' '0.05' '*' in expression specification (state=42000,code=40000)

Closing: 0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.hadoop:2181,h01hn02.hadoop:2181/tpcds_bin_partitioned_orc_5000;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2
STOP hdp2.6_hivetez_run_orc_5000_2_query24_sql_2017-04-18-22-56:  Tue Apr 18 22:56:11 CDT 2017
