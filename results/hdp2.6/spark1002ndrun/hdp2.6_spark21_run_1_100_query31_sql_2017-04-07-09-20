START hdp2.6_spark21_run_1_100_query31_sql_2017-04-07-09-20:  Fri Apr 7 09:20:52 CDT 2017
 /usr/hdp/current/spark2-client/bin/beeline -u "jdbc:hive2://h01hn02.hadoop:10016/tpcds_bin_partitioned_parquet_$SF" -n hive --incremental=true  -i settings/spark.settings -f sample-queries-tpcds/$1
Connecting to jdbc:hive2://h01hn02.hadoop:10016/tpcds_bin_partitioned_parquet_100
17/04/07 09:20:52 INFO Utils: Supplied authorities: h01hn02.hadoop:10016
17/04/07 09:20:52 INFO Utils: Resolved authority: h01hn02.hadoop:10016
17/04/07 09:20:52 INFO HiveConnection: Will try to open client transport with JDBC Uri: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bin_partitioned_parquet_100
Connected to: Spark SQL (version 2.1.0.2.6.0.0-598)
Driver: Hive JDBC (version 1.2.1.spark2.hdp)
Transaction isolation: TRANSACTION_REPEATABLE_READ
Running init script settings/spark.settings
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi> 
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi> 
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi> with ss as
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>  (select ca_county,d_qoy, d_year,sum(ss_ext_sales_price) as store_sales
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>  from store_sales,date_dim,customer_address
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>  where ss_sold_date_sk = d_date_sk
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>   and ss_addr_sk=ca_address_sk
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>  group by ca_county,d_qoy, d_year),
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>  ws as
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>  (select ca_county,d_qoy, d_year,sum(ws_ext_sales_price) as web_sales
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>  from web_sales,date_dim,customer_address
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>  where ws_sold_date_sk = d_date_sk
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>   and ws_bill_addr_sk=ca_address_sk
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>  group by ca_county,d_qoy, d_year)
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>  select
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>         ss1.ca_county
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>        ,ss1.d_year
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>        ,ws2.web_sales/ws1.web_sales web_q1_q2_increase
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>        ,ss2.store_sales/ss1.store_sales store_q1_q2_increase
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>        ,ws3.web_sales/ws2.web_sales web_q2_q3_increase
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>        ,ss3.store_sales/ss2.store_sales store_q2_q3_increase
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>  from
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>         ss ss1
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>        ,ss ss2
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>        ,ss ss3
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>        ,ws ws1
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>        ,ws ws2
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>        ,ws ws3
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>  where
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>     ss1.d_qoy = 1
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>     and ss1.d_year = 1998
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>     and ss1.ca_county = ss2.ca_county
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>     and ss2.d_qoy = 2
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>     and ss2.d_year = 1998
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>  and ss2.ca_county = ss3.ca_county
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>     and ss3.d_qoy = 3
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>     and ss3.d_year = 1998
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>     and ss1.ca_county = ws1.ca_county
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>     and ws1.d_qoy = 1
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>     and ws1.d_year = 1998
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>     and ws1.ca_county = ws2.ca_county
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>     and ws2.d_qoy = 2
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>     and ws2.d_year = 1998
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>     and ws1.ca_county = ws3.ca_county
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>     and ws3.d_qoy = 3
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>     and ws3.d_year =1998
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>     and case when ws1.web_sales > 0 then ws2.web_sales/ws1.web_sales else null end 
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>        > case when ss1.store_sales > 0 then ss2.store_sales/ss1.store_sales else null end
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>     and case when ws2.web_sales > 0 then ws3.web_sales/ws2.web_sales else null end
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>        > case when ss2.store_sales > 0 then ss3.store_sales/ss2.store_sales else null end
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>  order by web_q1_q2_increase;
Error: org.apache.thrift.transport.TTransportException: java.net.SocketException: Broken pipe (Write failed) (state=08S01,code=0)

Closing: 0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bin_partitioned_parquet_100
17/04/07 09:21:34 WARN TIOStreamTransport: Error closing output stream.
java.net.SocketException: Socket closed
	at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:116)
	at java.net.SocketOutputStream.write(SocketOutputStream.java:153)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
	at java.io.FilterOutputStream.close(FilterOutputStream.java:158)
	at org.apache.thrift.transport.TIOStreamTransport.close(TIOStreamTransport.java:110)
	at org.apache.thrift.transport.TSocket.close(TSocket.java:235)
	at org.apache.thrift.transport.TSaslTransport.close(TSaslTransport.java:402)
	at org.apache.thrift.transport.TSaslClientTransport.close(TSaslClientTransport.java:37)
	at org.apache.hive.jdbc.HiveConnection.close(HiveConnection.java:727)
	at org.apache.hive.beeline.Commands.close(Commands.java:991)
	at org.apache.hive.beeline.Commands.closeall(Commands.java:969)
	at org.apache.hive.beeline.BeeLine.close(BeeLine.java:826)
	at org.apache.hive.beeline.BeeLine.begin(BeeLine.java:773)
	at org.apache.hive.beeline.BeeLine.mainWithInputRedirection(BeeLine.java:484)
	at org.apache.hive.beeline.BeeLine.main(BeeLine.java:467)
Error: Error while cleaning up the server resources (state=,code=0)
STOP hdp2.6_spark21_run_1_100_query31_sql_2017-04-07-09-20:  Fri Apr 7 09:21:34 CDT 2017
