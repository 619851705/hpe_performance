START hdp2.6_spark21_run_1_100_query48_sql_2017-04-06-19-06:  Thu Apr 6 19:06:42 CDT 2017
 /usr/hdp/current/spark2-client/bin/beeline -u "jdbc:hive2://h01hn02.hadoop:10016/tpcds_bin_partitioned_parquet_$SF" -n hive --incremental=true  -i settings/spark.settings -f sample-queries-tpcds/$1
Connecting to jdbc:hive2://h01hn02.hadoop:10016/tpcds_bin_partitioned_parquet_100
17/04/06 19:06:43 INFO Utils: Supplied authorities: h01hn02.hadoop:10016
17/04/06 19:06:43 INFO Utils: Resolved authority: h01hn02.hadoop:10016
17/04/06 19:06:43 INFO HiveConnection: Will try to open client transport with JDBC Uri: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bin_partitioned_parquet_100
17/04/06 19:06:43 INFO HiveConnection: Could not open client transport with JDBC Uri: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bin_partitioned_parquet_100
17/04/06 19:06:43 INFO HiveConnection: Transport Used for JDBC connection: null
Error: Could not open client transport with JDBC Uri: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bin_partitioned_parquet_100: java.net.ConnectException: Connection refused (Connection refused) (state=08S01,code=0)
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)> 
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)> select sum (ss_quantity)
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>  from store_sales, store, customer_demographics, customer_address, date_dim
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>  where store.s_store_sk = store_sales.ss_store_sk
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>  and  store_sales.ss_sold_date_sk = date_dim.d_date_sk and d_year = 1998
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>  and  
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>  (
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   (
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>    customer_demographics.cd_demo_sk = store_sales.ss_cdemo_sk
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>    and 
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>    cd_marital_status = 'M'
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>    and 
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>    cd_education_status = '4 yr Degree'
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>    and 
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>    ss_sales_price between 100.00 and 150.00  
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>    )
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>  or
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   (
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   customer_demographics.cd_demo_sk = store_sales.ss_cdemo_sk
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>    and 
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>    cd_marital_status = 'M'
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>    and 
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>    cd_education_status = '4 yr Degree'
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>    and 
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>    ss_sales_price between 50.00 and 100.00   
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   )
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>  or 
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>  (
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   customer_demographics.cd_demo_sk = store_sales.ss_cdemo_sk
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   and 
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>    cd_marital_status = 'M'
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>    and 
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>    cd_education_status = '4 yr Degree'
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>    and 
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>    ss_sales_price between 150.00 and 200.00  
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>  )
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>  )
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>  and
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>  (
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   (
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   store_sales.ss_addr_sk = customer_address.ca_address_sk
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   and
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   ca_country = 'United States'
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   and
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   ca_state in ('KY', 'GA', 'NM')
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   and ss_net_profit between 0 and 2000  
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   )
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>  or
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   (store_sales.ss_addr_sk = customer_address.ca_address_sk
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   and
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   ca_country = 'United States'
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   and
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   ca_state in ('MT', 'OR', 'IN')
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   and ss_net_profit between 150 and 3000 
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   )
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>  or
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   (store_sales.ss_addr_sk = customer_address.ca_address_sk
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   and
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   ca_country = 'United States'
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   and
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   ca_state in ('WI', 'MO', 'WV')
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   and ss_net_profit between 50 and 25000 
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   )
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>  )
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)> ;
17/04/06 19:06:43 INFO Utils: Supplied authorities: h01hn02.hadoop:10016
17/04/06 19:06:43 INFO Utils: Resolved authority: h01hn02.hadoop:10016
17/04/06 19:06:43 INFO HiveConnection: Will try to open client transport with JDBC Uri: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bin_partitioned_parquet_100
17/04/06 19:06:43 INFO HiveConnection: Could not open client transport with JDBC Uri: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bin_partitioned_parquet_100
17/04/06 19:06:43 INFO HiveConnection: Transport Used for JDBC connection: null
No current connection

17/04/06 19:06:43 INFO Utils: Supplied authorities: h01hn02.hadoop:10016
17/04/06 19:06:43 INFO Utils: Resolved authority: h01hn02.hadoop:10016
17/04/06 19:06:43 INFO HiveConnection: Will try to open client transport with JDBC Uri: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bin_partitioned_parquet_100
17/04/06 19:06:43 INFO HiveConnection: Could not open client transport with JDBC Uri: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bin_partitioned_parquet_100
17/04/06 19:06:43 INFO HiveConnection: Transport Used for JDBC connection: null
Error: Could not open client transport with JDBC Uri: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bin_partitioned_parquet_100: java.net.ConnectException: Connection refused (Connection refused) (state=08S01,code=0)
STOP hdp2.6_spark21_run_1_100_query48_sql_2017-04-06-19-06:  Thu Apr 6 19:06:43 CDT 2017
