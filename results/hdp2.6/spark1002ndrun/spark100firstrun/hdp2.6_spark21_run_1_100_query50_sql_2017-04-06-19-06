START hdp2.6_spark21_run_1_100_query50_sql_2017-04-06-19-06:  Thu Apr 6 19:06:43 CDT 2017
 /usr/hdp/current/spark2-client/bin/beeline -u "jdbc:hive2://h01hn02.hadoop:10016/tpcds_bin_partitioned_parquet_$SF" -n hive --incremental=true  -i settings/spark.settings -f sample-queries-tpcds/$1
Connecting to jdbc:hive2://h01hn02.hadoop:10016/tpcds_bin_partitioned_parquet_100
17/04/06 19:06:44 INFO Utils: Supplied authorities: h01hn02.hadoop:10016
17/04/06 19:06:44 INFO Utils: Resolved authority: h01hn02.hadoop:10016
17/04/06 19:06:44 INFO HiveConnection: Will try to open client transport with JDBC Uri: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bin_partitioned_parquet_100
17/04/06 19:06:44 INFO HiveConnection: Could not open client transport with JDBC Uri: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bin_partitioned_parquet_100
17/04/06 19:06:44 INFO HiveConnection: Transport Used for JDBC connection: null
Error: Could not open client transport with JDBC Uri: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bin_partitioned_parquet_100: java.net.ConnectException: Connection refused (Connection refused) (state=08S01,code=0)
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)> 
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)> select  
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>    s_store_name
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   ,s_company_id
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   ,s_street_number
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   ,s_street_name
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   ,s_street_type
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   ,s_suite_number
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   ,s_city
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   ,s_county
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   ,s_state
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   ,s_zip
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   ,sum(case when (sr_returned_date_sk - ss_sold_date_sk <= 30 ) then 1 else 0 end)  as 30days 
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   ,sum(case when (sr_returned_date_sk - ss_sold_date_sk > 30) and 
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>                  (sr_returned_date_sk - ss_sold_date_sk <= 60) then 1 else 0 end )  as 3160days 
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   ,sum(case when (sr_returned_date_sk - ss_sold_date_sk > 60) and 
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>                  (sr_returned_date_sk - ss_sold_date_sk <= 90) then 1 else 0 end)  as 6190days 
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   ,sum(case when (sr_returned_date_sk - ss_sold_date_sk > 90) and
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>                  (sr_returned_date_sk - ss_sold_date_sk <= 120) then 1 else 0 end)  as 91120days 
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   ,sum(case when (sr_returned_date_sk - ss_sold_date_sk  > 120) then 1 else 0 end)  as 120days 
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)> from
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>    store_sales
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   ,store_returns
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   ,store
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   ,date_dim d1
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   ,date_dim d2
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)> where
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>     d2.d_year = 2000
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)> and d2.d_moy  = 9
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)> and store_sales.ss_ticket_number = store_returns.sr_ticket_number
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)> and store_sales.ss_item_sk = store_returns.sr_item_sk
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)> and store_sales.ss_sold_date_sk   = d1.d_date_sk
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)> and sr_returned_date_sk   = d2.d_date_sk
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)> and store_sales.ss_customer_sk = store_returns.sr_customer_sk
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)> and store_sales.ss_store_sk = store.s_store_sk
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)> group by
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>    s_store_name
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   ,s_company_id
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   ,s_street_number
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   ,s_street_name
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   ,s_street_type
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   ,s_suite_number
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   ,s_city
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   ,s_county
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   ,s_state
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>   ,s_zip
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)> order by s_store_name
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>         ,s_company_id
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>         ,s_street_number
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>         ,s_street_name
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>         ,s_street_type
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>         ,s_suite_number
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>         ,s_city
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>         ,s_county
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>         ,s_state
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>         ,s_zip
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)> limit 100;
17/04/06 19:06:44 INFO Utils: Supplied authorities: h01hn02.hadoop:10016
17/04/06 19:06:44 INFO Utils: Resolved authority: h01hn02.hadoop:10016
17/04/06 19:06:44 INFO HiveConnection: Will try to open client transport with JDBC Uri: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bin_partitioned_parquet_100
17/04/06 19:06:44 INFO HiveConnection: Could not open client transport with JDBC Uri: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bin_partitioned_parquet_100
17/04/06 19:06:44 INFO HiveConnection: Transport Used for JDBC connection: null
No current connection

17/04/06 19:06:44 INFO Utils: Supplied authorities: h01hn02.hadoop:10016
17/04/06 19:06:44 INFO Utils: Resolved authority: h01hn02.hadoop:10016
17/04/06 19:06:44 INFO HiveConnection: Will try to open client transport with JDBC Uri: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bin_partitioned_parquet_100
17/04/06 19:06:44 INFO HiveConnection: Could not open client transport with JDBC Uri: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bin_partitioned_parquet_100
17/04/06 19:06:44 INFO HiveConnection: Transport Used for JDBC connection: null
Error: Could not open client transport with JDBC Uri: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bin_partitioned_parquet_100: java.net.ConnectException: Connection refused (Connection refused) (state=08S01,code=0)
STOP hdp2.6_spark21_run_1_100_query50_sql_2017-04-06-19-06:  Thu Apr 6 19:06:44 CDT 2017
