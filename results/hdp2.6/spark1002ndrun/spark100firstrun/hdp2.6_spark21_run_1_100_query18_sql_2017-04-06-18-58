START hdp2.6_spark21_run_1_100_query18_sql_2017-04-06-18-58:  Thu Apr 6 18:58:37 CDT 2017
 /usr/hdp/current/spark2-client/bin/beeline -u "jdbc:hive2://h01hn02.hadoop:10016/tpcds_bin_partitioned_parquet_$SF" -n hive --incremental=true  -i settings/spark.settings -f sample-queries-tpcds/$1
Connecting to jdbc:hive2://h01hn02.hadoop:10016/tpcds_bin_partitioned_parquet_100
17/04/06 18:58:37 INFO Utils: Supplied authorities: h01hn02.hadoop:10016
17/04/06 18:58:37 INFO Utils: Resolved authority: h01hn02.hadoop:10016
17/04/06 18:58:37 INFO HiveConnection: Will try to open client transport with JDBC Uri: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bin_partitioned_parquet_100
Connected to: Spark SQL (version 2.1.0.2.6.0.0-598)
Driver: Hive JDBC (version 1.2.1.spark2.hdp)
Transaction isolation: TRANSACTION_REPEATABLE_READ
Running init script settings/spark.settings
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi> 
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi> 
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi> select  i_item_id,
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>         ca_country,
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>         ca_state, 
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>         ca_county,
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>         avg( cast(cs_quantity as decimal(12,2))) agg1,
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>         avg( cast(cs_list_price as decimal(12,2))) agg2,
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>         avg( cast(cs_coupon_amt as decimal(12,2))) agg3,
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>         avg( cast(cs_sales_price as decimal(12,2))) agg4,
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>         avg( cast(cs_net_profit as decimal(12,2))) agg5,
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>         avg( cast(c_birth_year as decimal(12,2))) agg6,
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>         avg( cast(cd1.cd_dep_count as decimal(12,2))) agg7
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>  from catalog_sales, date_dim, customer_demographics cd1, item, customer, customer_address, 
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>       customer_demographics cd2
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>  where catalog_sales.cs_sold_date_sk = date_dim.d_date_sk and
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>        catalog_sales.cs_item_sk = item.i_item_sk and
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>        catalog_sales.cs_bill_cdemo_sk = cd1.cd_demo_sk and
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>        catalog_sales.cs_bill_customer_sk = customer.c_customer_sk and
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>        cd1.cd_gender = 'M' and 
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>        cd1.cd_education_status = 'College' and
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>        customer.c_current_cdemo_sk = cd2.cd_demo_sk and
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>        customer.c_current_addr_sk = customer_address.ca_address_sk and
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>        c_birth_month in (9,5,12,4,1,10) and
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>        d_year = 2001 and
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>        ca_state in ('ND','WI','AL'
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>                    ,'NC','OK','MS','TN')
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>  group by i_item_id, ca_country, ca_state, ca_county with rollup
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>  order by ca_country,
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>         ca_state, 
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>         ca_county,
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi> i_item_id
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>  limit 100;
Error: org.apache.thrift.transport.TTransportException: java.net.SocketException: Broken pipe (Write failed) (state=08S01,code=0)

Closing: 0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bin_partitioned_parquet_100
17/04/06 19:06:28 WARN TIOStreamTransport: Error closing output stream.
java.net.SocketException: Socket closed
	at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:116)
	at java.net.SocketOutputStream.write(SocketOutputStream.java:153)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
	at java.io.FilterOutputStream.close(FilterOutputStream.java:158)
	at org.apache.thrift.transport.TIOStreamTransport.close(TIOStreamTransport.java:110)
	at org.apache.thrift.transport.TSocket.close(TSocket.java:235)
	at org.apache.thrift.transport.TSaslTransport.close(TSaslTransport.java:402)
	at org.apache.thrift.transport.TSaslClientTransport.close(TSaslClientTransport.java:37)
	at org.apache.hive.jdbc.HiveConnection.close(HiveConnection.java:727)
	at org.apache.hive.beeline.Commands.close(Commands.java:991)
	at org.apache.hive.beeline.Commands.closeall(Commands.java:969)
	at org.apache.hive.beeline.BeeLine.close(BeeLine.java:826)
	at org.apache.hive.beeline.BeeLine.begin(BeeLine.java:773)
	at org.apache.hive.beeline.BeeLine.mainWithInputRedirection(BeeLine.java:484)
	at org.apache.hive.beeline.BeeLine.main(BeeLine.java:467)
Error: Error while cleaning up the server resources (state=,code=0)
STOP hdp2.6_spark21_run_1_100_query18_sql_2017-04-06-18-58:  Thu Apr 6 19:06:28 CDT 2017
