START hdp2.6_spark21_run_1_100_query76_sql_2017-04-06-17-15:  Thu Apr 6 17:15:36 CDT 2017
 /usr/hdp/current/spark2-client/bin/beeline -u "jdbc:hive2://h01hn02.hadoop:10016/tpcds_bin_partitioned_parquet_$SF" -n hive --incremental=true  -i settings/spark.settings -f sample-queries-tpcds/$1
Connecting to jdbc:hive2://h01hn02.hadoop:10016/tpcds_bin_partitioned_parquet_100
17/04/06 17:15:37 INFO Utils: Supplied authorities: h01hn02.hadoop:10016
17/04/06 17:15:37 INFO Utils: Resolved authority: h01hn02.hadoop:10016
17/04/06 17:15:37 INFO HiveConnection: Will try to open client transport with JDBC Uri: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bin_partitioned_parquet_100
Connected to: Spark SQL (version 2.1.0.2.6.0.0-598)
Driver: Hive JDBC (version 1.2.1.spark2.hdp)
Transaction isolation: TRANSACTION_REPEATABLE_READ
Running init script settings/spark.settings
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi> 
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi> 
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi> 
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi> select  channel, col_name, d_year, d_qoy, i_category, COUNT(*) sales_cnt, SUM(ext_sales_price) sales_amt FROM (
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>         SELECT 'store' as channel, 'ss_addr_sk' col_name, d_year, d_qoy, i_category, ss_ext_sales_price ext_sale s_price
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>          FROM store_sales, item, date_dim
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>          WHERE ss_addr_sk IS NULL
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>            AND store_sales.ss_sold_date_sk=date_dim.d_date_sk
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>            AND store_sales.ss_item_sk=item.i_item_sk
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>         UNION ALL
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>         SELECT 'web' as channel, 'ws_web_page_sk' col_name, d_year, d_qoy, i_category, ws_ext_sales_price ext_sa les_price
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>          FROM web_sales, item, date_dim
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>          WHERE ws_web_page_sk IS NULL
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>            AND web_sales.ws_sold_date_sk=date_dim.d_date_sk
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>            AND web_sales.ws_item_sk=item.i_item_sk
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>         UNION ALL
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>         SELECT 'catalog' as channel, 'cs_warehouse_sk' col_name, d_year, d_qoy, i_category, cs_ext_sales_price e xt_sales_price
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>          FROM catalog_sales, item, date_dim
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>          WHERE cs_warehouse_sk IS NULL
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>            AND catalog_sales.cs_sold_date_sk=date_dim.d_date_sk
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>            AND catalog_sales.cs_item_sk=item.i_item_sk) foo
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi> GROUP BY channel, col_name, d_year, d_qoy, i_category
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi> ORDER BY channel, col_name, d_year, d_qoy, i_category
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi> limit 100;
Error: org.apache.spark.SparkException: Job 299 cancelled because SparkContext was shut down (state=,code=0)

Closing: 0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bin_partitioned_parquet_100
STOP hdp2.6_spark21_run_1_100_query76_sql_2017-04-06-17-15:  Thu Apr 6 17:16:42 CDT 2017
