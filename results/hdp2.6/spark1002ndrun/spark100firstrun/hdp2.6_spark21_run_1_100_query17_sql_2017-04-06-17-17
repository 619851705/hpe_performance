START hdp2.6_spark21_run_1_100_query17_sql_2017-04-06-17-17:  Thu Apr 6 17:17:36 CDT 2017
 /usr/hdp/current/spark2-client/bin/beeline -u "jdbc:hive2://h01hn02.hadoop:10016/tpcds_bin_partitioned_parquet_$SF" -n hive --incremental=true  -i settings/spark.settings -f sample-queries-tpcds/$1
Connecting to jdbc:hive2://h01hn02.hadoop:10016/tpcds_bin_partitioned_parquet_100
17/04/06 17:17:36 INFO Utils: Supplied authorities: h01hn02.hadoop:10016
17/04/06 17:17:36 INFO Utils: Resolved authority: h01hn02.hadoop:10016
17/04/06 17:17:36 INFO HiveConnection: Will try to open client transport with JDBC Uri: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bin_partitioned_parquet_100
Error: Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.main(HiveThriftServer2.scala)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:745)
org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187)
org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212)
org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)
org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

The currently active SparkContext was created at:

(No active SparkContext.) (state=,code=0)
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)> select  i_item_id
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>        ,i_item_desc
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>        ,s_state
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>        ,count(ss_quantity) as store_sales_quantitycount
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>        ,avg(ss_quantity) as store_sales_quantityave
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>        ,stddev_samp(ss_quantity) as store_sales_quantitystdev
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>        ,stddev_samp(ss_quantity)/avg(ss_quantity) as store_sales_quantitycov
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>        ,count(sr_return_quantity) as_store_returns_quantitycount
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>        ,avg(sr_return_quantity) as_store_returns_quantityave
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>        ,stddev_samp(sr_return_quantity) as_store_returns_quantitystdev
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>        ,stddev_samp(sr_return_quantity)/avg(sr_return_quantity) as store_returns_quantitycov
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>        ,count(cs_quantity) as catalog_sales_quantitycount ,avg(cs_quantity) as catalog_sales_quantityav e
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>        ,stddev_samp(cs_quantity)/avg(cs_quantity) as catalog_sales_quantitystdev
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>        ,stddev_samp(cs_quantity)/avg(cs_quantity) as catalog_sales_quantitycov
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>  from store_sales
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>      ,store_returns
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>      ,catalog_sales
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>      ,date_dim d1
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>      ,date_dim d2
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>      ,date_dim d3
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>      ,store
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>      ,item
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>  where d1.d_quarter_name = '2000Q1'
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>    and d1.d_date_sk = store_sales.ss_sold_date_sk
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>    and item.i_item_sk = store_sales.ss_item_sk
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>    and store.s_store_sk = store_sales.ss_store_sk
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>    and store_sales.ss_customer_sk = store_returns.sr_customer_sk
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>    and store_sales.ss_item_sk = store_returns.sr_item_sk
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>    and store_sales.ss_ticket_number = store_returns.sr_ticket_number
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>    and store_returns.sr_returned_date_sk = d2.d_date_sk
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>    and d2.d_quarter_name in ('2000Q1','2000Q2','2000Q3')
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>    and store_returns.sr_customer_sk = catalog_sales.cs_bill_customer_sk
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>    and store_returns.sr_item_sk = catalog_sales.cs_item_sk
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>    and catalog_sales.cs_sold_date_sk = d3.d_date_sk
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>    and d3.d_quarter_name in ('2000Q1','2000Q2','2000Q3')
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>  group by i_item_id
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>          ,i_item_desc
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>          ,s_state
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>  order by i_item_id
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>          ,i_item_desc
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>          ,s_state
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)> limit 100;
17/04/06 17:17:36 INFO Utils: Supplied authorities: h01hn02.hadoop:10016
17/04/06 17:17:36 INFO Utils: Resolved authority: h01hn02.hadoop:10016
17/04/06 17:17:36 INFO HiveConnection: Will try to open client transport with JDBC Uri: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bin_partitioned_parquet_100
No current connection

17/04/06 17:17:36 INFO Utils: Supplied authorities: h01hn02.hadoop:10016
17/04/06 17:17:36 INFO Utils: Resolved authority: h01hn02.hadoop:10016
17/04/06 17:17:36 INFO HiveConnection: Will try to open client transport with JDBC Uri: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bin_partitioned_parquet_100
Error: Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.main(HiveThriftServer2.scala)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:745)
org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187)
org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212)
org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)
org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

The currently active SparkContext was created at:

(No active SparkContext.) (state=,code=0)
STOP hdp2.6_spark21_run_1_100_query17_sql_2017-04-06-17-17:  Thu Apr 6 17:17:36 CDT 2017
