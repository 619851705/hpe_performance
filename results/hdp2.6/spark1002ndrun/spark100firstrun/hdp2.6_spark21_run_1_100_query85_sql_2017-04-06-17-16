START hdp2.6_spark21_run_1_100_query85_sql_2017-04-06-17-16:  Thu Apr 6 17:16:52 CDT 2017
 /usr/hdp/current/spark2-client/bin/beeline -u "jdbc:hive2://h01hn02.hadoop:10016/tpcds_bin_partitioned_parquet_$SF" -n hive --incremental=true  -i settings/spark.settings -f sample-queries-tpcds/$1
Connecting to jdbc:hive2://h01hn02.hadoop:10016/tpcds_bin_partitioned_parquet_100
17/04/06 17:16:52 INFO Utils: Supplied authorities: h01hn02.hadoop:10016
17/04/06 17:16:52 INFO Utils: Resolved authority: h01hn02.hadoop:10016
17/04/06 17:16:52 INFO HiveConnection: Will try to open client transport with JDBC Uri: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bin_partitioned_parquet_100
Error: Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.main(HiveThriftServer2.scala)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:745)
org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187)
org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212)
org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)
org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

The currently active SparkContext was created at:

(No active SparkContext.) (state=,code=0)
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)> 
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)> select  substr(r_reason_desc,1,20) as r
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>        ,avg(ws_quantity) wq
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>        ,avg(wr_refunded_cash) ref
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>        ,avg(wr_fee) fee
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>  from web_sales, web_returns, web_page, customer_demographics cd1,
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>       customer_demographics cd2, customer_address, date_dim, reason 
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>  where web_sales.ws_web_page_sk = web_page.wp_web_page_sk
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>    and web_sales.ws_item_sk = web_returns.wr_item_sk
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>    and web_sales.ws_order_number = web_returns.wr_order_number
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>    and web_sales.ws_sold_date_sk = date_dim.d_date_sk and d_year = 1998
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>    and cd1.cd_demo_sk = web_returns.wr_refunded_cdemo_sk 
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>    and cd2.cd_demo_sk = web_returns.wr_returning_cdemo_sk
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>    and customer_address.ca_address_sk = web_returns.wr_refunded_addr_sk
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>    and reason.r_reason_sk = web_returns.wr_reason_sk
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>    and
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>    (
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>     (
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>      cd1.cd_marital_status = 'M'
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>      and
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>      cd1.cd_marital_status = cd2.cd_marital_status
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>      and
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>      cd1.cd_education_status = '4 yr Degree'
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>      and 
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>      cd1.cd_education_status = cd2.cd_education_status
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>      and
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>      ws_sales_price between 100.00 and 150.00
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>     )
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>    or
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>     (
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>      cd1.cd_marital_status = 'D'
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>      and
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>      cd1.cd_marital_status = cd2.cd_marital_status
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>      and
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>      cd1.cd_education_status = 'Primary' 
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>      and
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>      cd1.cd_education_status = cd2.cd_education_status
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>      and
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>      ws_sales_price between 50.00 and 100.00
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>     )
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>    or
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>     (
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>      cd1.cd_marital_status = 'U'
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>      and
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>      cd1.cd_marital_status = cd2.cd_marital_status
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>      and
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>      cd1.cd_education_status = 'Advanced Degree'
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>      and
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>      cd1.cd_education_status = cd2.cd_education_status
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>      and
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>      ws_sales_price between 150.00 and 200.00
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>     )
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>    )
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>    and
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>    (
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>     (
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>      ca_country = 'United States'
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>      and
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>      ca_state in ('KY', 'GA', 'NM')
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>      and ws_net_profit between 100 and 200  
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>     )
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>     or
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>     (
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>      ca_country = 'United States'
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>      and
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>      ca_state in ('MT', 'OR', 'IN')
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>      and ws_net_profit between 150 and 300  
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>     )
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>     or
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>     (
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>      ca_country = 'United States'
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>      and
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>      ca_state in ('WI', 'MO', 'WV')
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>      and ws_net_profit between 50 and 250  
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>     )
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)>    )
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)> group by r_reason_desc
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)> order by r, wq, ref, fee
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi (closed)> limit 100;
17/04/06 17:16:53 INFO Utils: Supplied authorities: h01hn02.hadoop:10016
17/04/06 17:16:53 INFO Utils: Resolved authority: h01hn02.hadoop:10016
17/04/06 17:16:53 INFO HiveConnection: Will try to open client transport with JDBC Uri: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bin_partitioned_parquet_100
No current connection

17/04/06 17:16:53 INFO Utils: Supplied authorities: h01hn02.hadoop:10016
17/04/06 17:16:53 INFO Utils: Resolved authority: h01hn02.hadoop:10016
17/04/06 17:16:53 INFO HiveConnection: Will try to open client transport with JDBC Uri: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bin_partitioned_parquet_100
Error: Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.main(HiveThriftServer2.scala)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:745)
org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187)
org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212)
org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)
org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

The currently active SparkContext was created at:

(No active SparkContext.) (state=,code=0)
STOP hdp2.6_spark21_run_1_100_query85_sql_2017-04-06-17-16:  Thu Apr 6 17:16:53 CDT 2017
