START hdp2.6_hivetez_run_orc_1000_1_query72_sql_2017-04-12-07-24:  Wed Apr 12 07:24:04 CDT 2017
beeline -u "jdbc:hive2://h01mgt.hadoop:2181,h01hn01.hadoop:2181,h01hn02.hadoop:2181/tpcds_bin_partitioned_${FILETYPE}_${SF};serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2" -n hive --incremental=true -i settings/${SETTINGS}.settings -f sample-queries-tpcds/$1
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set tez.container.max.java.heap.fraction=0.8;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> --set hive.log.level=WARN;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.tez.exec.print.summary=true;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> #set hive.tez.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.tez.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.enforce.bucketing=true;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.enforce.sorting=true;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.tez.container.size=8192; 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set tez.am.resource.memory.mb=8192;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set tez.task.resource.memory.mb=8192; 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set tez.runtime.io.sort.mb=3277; 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.auto.convert.join.noconditionaltask=true; 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.auto.convert.join.noconditionaltask.size=2834678415; 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set tez.runtime.unordered.output.buffer.size-mb=819; 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> #set tez.grouping.min-size=1073741824; 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> #set tez.grouping.max-size=2147483648; 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.exec.parallel=true; 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.exec.parallel.thread.number=16; 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.tez.auto.reducer.parallelism=true; 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.exec.reducers.bytes.per.reducer=268435456; 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.optimize.reducededuplication.min.reducer = 1 ;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> --set tez.queue.name=llap;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.exec.dynamic.partition.mode=nonstrict;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.exec.max.dynamic.partitions.pernode=100000;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.exec.max.dynamic.partitions=100000;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.exec.max.created.files=1000000;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.exec.parallel=true;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> --set hive.exec.reducers.max=2000;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.stats.autogather=true;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.optimize.sort.dynamic.partition=true;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.stats.autogather=true;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.execution.engine=tez;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.cbo.enable=true;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.compute.query.using.stats=true;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.stats.fetch.column.stats=true;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.stats.fetch.partition.stats=true;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.vectorized.execution.enabled=true;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.vectorized.execution.reduce.enabled = true;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.vectorized.execution.reduce.groupby.enabled = true;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set mapred.job.reduce.input.buffer.percent=0.0;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> --set mapreduce.input.fileinputformat.split.minsize=1073741824;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> --set mapreduce.input.fileinputformat.split.maxsize=2147483648;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> --set mapreduce.input.fileinputformat.split.minsize.per.node=1073741824;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> --set mapreduce.input.fileinputformat.split.minsize.per.rack=1073741824;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> --set hive.tez.java.opts=-XX:+PrintGCDetails -verbose:gc -XX:+PrintGCTimeStamps -XX:+U seNUMA -XX:+UseG1GC -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set tez.runtime.empty.partitions.info-via-events.enabled=true;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set tez.runtime.report.partition.stats=true;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> --set mapred.map.tasks=6;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.limit.pushdown.memory.usage=0.04;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.map.aggr=true;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.mapjoin.bucket.cache.size=10000;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.mapred.reduce.tasks.speculative.execution=false;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.metastore.cache.pinobjtypes=Table,Database,Type,FieldSchema,Order;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.metastore.client.socket.timeout=60;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> set hive.metastore.execute.setugi=true;
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> select  i_item_desc
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>       ,w_warehouse_name
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>       ,d1.d_week_seq
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>       ,count(case when p_promo_sk is null then 1 else 0 end) no_promo
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>       ,count(case when p_promo_sk is not null then 1 else 0 end) promo
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>       ,count(*) total_cnt
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> from catalog_sales
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> join inventory on (catalog_sales.cs_item_sk = inventory.inv_item_sk)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> join warehouse on (warehouse.w_warehouse_sk=inventory.inv_warehouse_sk)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> join item on (item.i_item_sk = catalog_sales.cs_item_sk)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> join customer_demographics on (catalog_sales.cs_bill_cdemo_sk = customer_demographics. cd_demo_sk)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> join household_demographics on (catalog_sales.cs_bill_hdemo_sk = household_demographic s.hd_demo_sk)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> join date_dim d1 on (catalog_sales.cs_sold_date_sk = d1.d_date_sk)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> join date_dim d2 on (inventory.inv_date_sk = d2.d_date_sk)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> join date_dim d3 on (catalog_sales.cs_ship_date_sk = d3.d_date_sk)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> left outer join promotion on (catalog_sales.cs_promo_sk=promotion.p_promo_sk)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> left outer join catalog_returns on (catalog_returns.cr_item_sk = catalog_sales.cs_item _sk and catalog_returns.cr_order_number = catalog_sales.cs_order_number)
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> where d1.d_week_seq = d2.d_week_seq
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>   and inv_quantity_on_hand < cs_quantity 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>   and d3.d_date > d1.d_date + 5
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>   and hd_buy_potential = '1001-5000'
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>   and d1.d_year = 2001
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>   and hd_buy_potential = '1001-5000'
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>   and cd_marital_status = 'M'
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha>   and d1.d_year = 2001
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> group by i_item_desc,w_warehouse_name,d1.d_week_seq
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> order by total_cnt desc, i_item_desc, w_warehouse_name, d_week_seq
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> limit 100;
+--------------+-------------------+----------------+-----------+--------+------------+--+
| i_item_desc  | w_warehouse_name  | d1.d_week_seq  | no_promo  | promo  | total_cnt  |
+--------------+-------------------+----------------+-----------+--------+------------+--+
+--------------+-------------------+----------------+-----------+--------+------------+--+
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> 
0: jdbc:hive2://h01mgt.hadoop:2181,h01hn01.ha> 
STOP hdp2.6_hivetez_run_orc_1000_1_query72_sql_2017-04-12-07-24:  Wed Apr 12 08:04:48 CDT 2017
