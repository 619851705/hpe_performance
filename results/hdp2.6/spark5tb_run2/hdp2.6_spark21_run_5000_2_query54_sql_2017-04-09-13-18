START hdp2.6_spark21_run_5000_2_query54_sql_2017-04-09-13-18:  Sun Apr 9 13:18:55 CDT 2017
 /usr/hdp/current/spark2-client/bin/beeline -u "jdbc:hive2://h01hn02.hadoop:10016/tpcds_bin_partitioned_parquet_$SF" -n hive --incremental=true  -i settings/spark.settings -f sample-queries-tpcds/$1
Connecting to jdbc:hive2://h01hn02.hadoop:10016/tpcds_bin_partitioned_parquet_5000
17/04/09 13:18:55 INFO Utils: Supplied authorities: h01hn02.hadoop:10016
17/04/09 13:18:55 INFO Utils: Resolved authority: h01hn02.hadoop:10016
17/04/09 13:18:55 INFO HiveConnection: Will try to open client transport with JDBC Uri: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bin_partitioned_parquet_5000
Connected to: Spark SQL (version 2.1.0.2.6.0.0-598)
Driver: Hive JDBC (version 1.2.1.spark2.hdp)
Transaction isolation: TRANSACTION_REPEATABLE_READ
Running init script settings/spark.settings
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi> 
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi> with my_customers as (
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>  select  c_customer_sk
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>         , c_current_addr_sk
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>  from   
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>         ( select cs_sold_date_sk sold_date_sk,
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>                  cs_bill_customer_sk customer_sk,
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>                  cs_item_sk item_sk
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>           from   catalog_sales
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>           union all
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>           select ws_sold_date_sk sold_date_sk,
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>                  ws_bill_customer_sk customer_sk,
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>                  ws_item_sk item_sk
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>           from   web_sales
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>          ) cs_or_ws_sales,
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>          item,
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>          date_dim,
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>          customer
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>  where   sold_date_sk = d_date_sk
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>          and item_sk = i_item_sk
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>          and i_category = 'Jewelry'
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>          and i_class = 'football'
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>          and c_customer_sk = cs_or_ws_sales.customer_sk
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>          and d_moy = 3
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>          and d_year = 2000
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>          group by  c_customer_sk
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>         , c_current_addr_sk
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>  )
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>  , my_revenue as (
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>  select c_customer_sk,
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>         sum(ss_ext_sales_price) as revenue
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>  from   my_customers,
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>         store_sales,
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>         customer_address,
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>         store,
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>         date_dim
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>  where  c_current_addr_sk = ca_address_sk
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>         and ca_county = s_county
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>         and ca_state = s_state
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>         and ss_sold_date_sk = d_date_sk
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>         and c_customer_sk = ss_customer_sk
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>         and d_month_seq between (1203)
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>                            and  (1205)
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>  group by c_customer_sk
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>  )
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>  , segments as
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>  (select cast((revenue/50) as int) as segment
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>   from   my_revenue
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>  )
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>   select  segment, count(*) as num_customers, segment*50 as segment_base
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>  from segments
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>  group by segment
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>  order by segment, num_customers
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi>  limit 100;+-------------+-----------------+--------------+--+
|   segment   |  num_customers  | segment_base |
+-------------+-----------------+--------------+--+
+-------------+-----------------+--------------+--+
No rows selected (56.151 seconds)
0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bi> 
Closing: 0: jdbc:hive2://h01hn02.hadoop:10016/tpcds_bin_partitioned_parquet_5000
STOP hdp2.6_spark21_run_5000_2_query54_sql_2017-04-09-13-18:  Sun Apr 9 13:19:52 CDT 2017
